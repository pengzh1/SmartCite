<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\Work\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-07-07T10:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Review Topic Discovery with Phrases using the Pólya Urn Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 23-29 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geli</forename><surname>Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Illi-nois at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illi-nois at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@cs.uic.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illi-nois at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Review Topic Discovery with Phrases using the Pólya Urn Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
						<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers <address><addrLine>Dublin, Ireland</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="667" to="676"/>
							<date type="published">August 23-29 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Topic modelling has been popularly used to discover latent topics from text documents. Most existing models work on individual words. That is, they treat each topic as a distribution over words. However, using only individual words has several shortcomings. First, it increases the co-occurrences of words which may be incorrect because a phrase with two words is not equivalent to two separate words. These extra and often incorrect co-occurrences result in poorer output topics. A multi-word phrase should be treated as one term by itself. Second, individual words are often difficult to use in practice because the meaning of a word in a phrase and the meaning of a word in isolation can be quite different. Third, topics as a list of individual words are also difficult to understand by users who are not domain experts and do not have any knowledge of topic models. In this paper, we aim to solve these problems by considering phrases in their natural form. One simple way to include phrases in topic modelling is to treat each phrase as a single term. However, this method is not ideal because the meaning of a phrase is often related to its composite words. That information is lost. This paper proposes to use the generalized Pólya Urn (GPU) model to solve the problem, which gives superior results. GPU enables the connection of a phrase with its content words naturally. Our experimental results using 32 review datasets show that the proposed approach is highly effective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models such as LDA <ref type="bibr" target="#b1">(Blei et al., 2003)</ref> and pSLA <ref type="bibr" target="#b7">(Hofmann 1999)</ref> and their extensions have been popularly used to find topics in text documents. These models are mostly governed by the phenomenon called "higher-order co-occurrence" <ref type="bibr" target="#b6">(Heinrich 2009</ref>), i.e., how often terms co-occur in different contexts. Word w1 co-occurring with word w2 which in turn co-occurs with word w3 denotes a second-order co-occurrence between w1 and w3. Almost all these models regard each topic as a distribution over words. The words under each topic are often sorted according to their associated probabilities. Those top ranked words are used to represent the topic. However, this representation of topics as a list of individual words has some 1 major shortcomings:</p><p>• Topics are often difficult to understand or interpret by users unless they are domain experts and also knowledgeable about topic models. In most real-life situations, these are not the case. In some of our applications, we show users several good topics, but they have no idea what they are because many domain phrases cannot be split to individual words. For example, "battery" and "life" are put under the same topic, which is not bad. But the users wondered why "battery" and "life" are the same because they thought words under a topic should somehow have similar meanings. We had to explain that it is due to "battery life." As another example, sentences such as "This hotel has a very nice sandy beach" may cause a topic model to put "hotel" and "sandy" in a topic, which is not wrong but again it is hard to understand by a user who may not be able to connect the two words. Thus in order to interpret topics well, the user must know the phrases (they are split into individual words) that may be used in a domain and how words may be associated with each other. To make the matters worse, in most cases, the topics generated from a topic model are not perfect. There are some wrong words under a topic, which make the interpretation even harder.</p><p>• Individual words are difficult to use in practice because in some cases a word under a topic may not have its intended meaning for the topic in a particular sentence context. This can cause many mistakes. For example, in sentiment analysis of product reviews, a topic is often regarded as a set of words indicating a product feature or attribute. This is not true in many cases. For example, if "battery" and "life" are put in one topic, when the system sees "life," it assumes it is related to "battery." But in the sentence "The life expectancy of the machine is about 2 years," this "life" has nothing to do with battery or battery life. This causes an error. If the system can directly use phrases, "battery life" and "life expectancy," the error will not occur.</p><p>• Splitting phrases into multiple individual words causes extra co-occurrences that may result in poor or wrong topics involving other words. For example, due to sentences like "Beach staffs are rude" and "The hotel has a nice sandy beach," a topic model may put "staff" and "sandy" under a topic for staff and/or put "beach" and "rude" together under the topic of beach views. Based on our experiences in opinion mining and social media mining, these are major issues with topic models. We believe that they must be dealt with before wide spread adaptation of topic models in real-life applications. In this paper, we make an attempt to solve this problem. We will use term to represent both word and phrase, and use word or phrase when we want to distinguish them.</p><p>One obvious way to consider phrases is to use a natural language parser to find all phrases and then treat each phrase as one term, e.g., "battery life," "sandy beach" and "beach staff." However, the problem with this approach is that it may lose the connection of many related words or phrases in a topic. For example, under the topic for beach, we may not find "sandy beach" because there is no co-occurrence of "sandy beach" and "beach" if we treat "sandy beach" as a single term. This is clearly not a good solution as it may miss a lot of topical terms (words or phrases) for a topic. It can also result in poor topics due to the loss of co-occurrences.</p><p>Another obvious solution is to use individual words as they are, but add an extra term representing the phrase. For example, we can turn the sentence "This hotel has a nice sandy beach" to "This hotel has a nice sandy beach &lt;sandy beach&gt;." This solution helps deal with the problem of losing co-occurrences to some extent, but because the words are still treated individually, the three problems discussed above still exist, although the phrase "sandy beach" now can show up in some topics. However, due to the fact that phrases are obviously less frequent than individual words, they may be ranked very low, which make little difference to solving the three problems.</p><p>In this paper, we propose a novel approach to solve the problem, which is based on the generalized Pólya urn (GPU) model <ref type="bibr" target="#b16">(Mahmoud 2008)</ref>. GPU was first introduced into LDA in <ref type="bibr" target="#b15">(Mimno et al., 2011)</ref> to concentrate words with high co-document frequency. However, <ref type="bibr" target="#b15">Mimno et al. (2011)</ref> and other researchers <ref type="bibr" target="#b4">Chen et al., (2013)</ref> still use them in the framework of individual words. In the GPU model, we can deal with the problems above by treating phrases as individual terms and allowing their component words to have some connections or co-occurrences with them. Furthermore, we can push phrases up in a topic as phrases are important for understanding but are usually less frequent than individual words and ranked low in a topic. The intuition here is that when we see a phrase, we also see a small fraction of their component words; and when we see each individual word, we also see a small fraction of its related phrases. Further, in a phrase not all words are equally important. For example, in "hotel staff", "staff" is more important as it is the head noun, which represents the semantic category of the phrase.</p><p>Our experiments are conducted using online review collections from 32 domains. We will see that the proposed method produces significantly better results both quantitatively based on the statistical measure of topic coherence and qualitatively based on human labeling of topics and topical terms.</p><p>In summary, this paper makes the following contributions: 1. It proposes to consider phrases in topic models, which as we have explained above, is important for accurate topic generation, the use of the resulting topics and human interpretation. As we will see in Section 2, although some prior works exist, they are based on n-grams . They are different from our approach. N-grams can generate many non-understandable phrases. Furthermore, due to infrequency of n-grams (much less frequent than individual words), typically a huge amount of data is needed in order to produce reasonable topics, which many applications simply do not have. 2. It proposes to use the generalized Pólya Urn (GPU) model to deal with the problems arising in considering phrases. To the best of our knowledge, the GPU model has not been used in the context of phrases. This model not only generates better topics, but also rank phrases relatively high in their topics, which greatly helps understanding of the generated topics. 3. Comprehensive experiments conducted using product and service review collections from 32 domains demonstrate the effectiveness of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>GPU was first introduced to topic modelling in <ref type="bibr" target="#b15">(Mimno et al., 2011)</ref>, in which GPU is used to concentrate words with high co-document frequency based on corpus-specific co-occurrence statistics. <ref type="bibr" target="#b4">Chen et al. (2013)</ref> applied GPU to deal with the adverse effect of using prior domain knowledge in topic modeling by increasing the counts of rare words in the knowledge sets. However, these works still use only individual words. Topics in most topic models like LDA are unigram distributions over words and assume words to be exchangeable at the word level. However, there exists some work that tries to take word order into consideration by including n-gram language models. Wallach <ref type="formula" target="#formula_2">(2006)</ref> proposed the Bigram Topic Model (BTM) which integrates bigram statistics with topic-based approaches to document modeling. <ref type="bibr" target="#b22">Wang et al. (2007)</ref> proposed the Topical N-gram Model (TNG), which is a generalization of the BTM. It generates words in their textual order by first sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Although the "bag-of-words" assumption does not always hold in real-life applications, it offers a great computational advantage over more complex models taking word order into account for discovering significant ngrams. Our approach is different from these works in two ways. First, we still follow the "bag-of-words" or rather "bag-of-terms" assumption. Second, we find actual phrases rather than just n-grams. Most ngrams are still hard to understand because they are not natural phrases.</p><p>Blei and Lafferty (2009), <ref type="bibr" target="#b12">Liu et al. (2010)</ref> and <ref type="bibr" target="#b24">Zhao et al. (2011)</ref> also try to extract keyphrases from texts. Their methods, however, are very different because they identify multi-word phrases using relevance and likelihood scores in the post-processing step based on the discovered topical unigrams.  and  all try to include n-grams to enhance the expressiveness of their models while preserving the advantages of "bag-of-words" assumption, which has a similar idea as our paper. However, as we point out in the introduction, this way of including phrases/n-grams suffers from several shortcomings. Solving these problems is the goal of our paper.</p><p>Finally, since we use product reviews as our datasets, our work is also related to opinion mining using topic models, e.g. <ref type="bibr" target="#b17">(Mei et al., 2007;</ref><ref type="bibr" target="#b11">Lu and Zhai, 2008;</ref><ref type="bibr" target="#b19">Titov and McDonald, 2008;</ref><ref type="bibr" target="#b23">Zhao et al., 2010;</ref><ref type="bibr" target="#b18">Sauper and Barzilay, 2013;</ref><ref type="bibr" target="#b9">Lin and He, 2009;</ref><ref type="bibr" target="#b8">Jo and Oh, 2011)</ref>. However, none of these models uses phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>We start by briefly reviewing the Latent Dirichlet Allocation (LDA) model <ref type="bibr" target="#b1">(Blei et al., 2003)</ref>. Then we describe the simple Pólya urn (SPU) model, which is embedded in LDA. After that, we present the generalized Pólya urn (GPU) model and discuss how it can be applied to our context. The proposed model uses GPU for its inference. It shares the same graphical model as LDA. However, the GPU inference mechanism is very different from that of LDA, which cannot be reflected in the graphical model or the generative process as it only helps to infer more desirable posterior distributions of topic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Latent Dirichlet Allocation</head><p>LDA is a generative probabilistic model for a document collection. It assumes that documents are represented as a mixture of latent topics, and each latent topic is characterized by a distribution over terms. In order to generate a term ( ) in document , where is its position, we first draw a discrete topic assignment ( ) from a document-specific distribution over topics , which is drawn from a prior Dirichlet distribution with hyperparameter . Then we draw a term from the topic-specific distribution over the vocabulary ( ) , which is drawn from a prior Dirichlet distribution with hyperparameter .</p><p>For inference, instead of directly estimating and , Gibbs sampling is used to approximate them based on the posterior estimates of latent topic assignment . The Gibbs sampling procedure considers each term in the documents in turn, and estimates the probability of assigning the current term to each topic, conditioned on the topic assignments to all other terms. <ref type="bibr" target="#b5">Griffiths and Steyvers (2004)</ref> showed this could be calculated by: . All these counts exclude the current term.</p><formula xml:id="formula_0">( ( ) = | − , , , , ) ∝ | + + × ( ) | + +<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simple Pólya Urn Model</head><p>Traditionally, the Pólya urn model is designed in the context of colored balls and urns. In the context of topic models, a term can be seen as a ball of a certain color and the urn contains a mixture of balls with various colors. The classic topic-word (or topic-term) distribution can be reflected by the color proportion of balls in the urn. LDA follows the simple Pólya urn (SPU) model, which works as follows: when a ball of a particular color is drawn from an urn, that ball is put back to the urn along with another ball of the same color. This process corresponds to assigning a topic to a term in the Gibbs sampler of LDA.</p><p>Based on the topic-specific "collapsed" probability of a term given topic ,</p><formula xml:id="formula_1">| + +</formula><p>, which is essentially the second ratio in (1), drawing a term will only increase the probability of seeing in the future sampling process. This self-reinforcing property is known as "the rich get richer". In the next subsection, we will introduce the generalized Pólya urn (GPU) model, which increases the probability of seeing certain other terms when we sample a term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalized Pólya Urn Model</head><p>The generalized Pólya urn (GPU) model differs from SPU in that, when a ball of a certain color is drawn, two balls of that color is put back along with a certain number of balls of some other colors. Unlike SPU, GPU sampling not only allows us to see a ball of the same color again with higher probability, but also increases the probability of seeing balls with certain other colors. These additional balls of certain other colors added to the urn increase their proportions in the urn. We call this the promotion of these colored balls. Applying the idea, there are two directions of promotion in our application (Note that in each sentence, we need to identify each phrase, but do not need to add any extra information): 1. Word to phrase: When an individual word is assigned to a topic (analogous to drawing a ball of a certain color), each phrase containing the word will be promoted, meaning that the phrase will be added to the same topic with a small count. That is, a fraction of the phrase will be assigned to the topic. This is justified because it is reasonable to assume that the phrase is related to the word to some extent in meaning. 2. Phrase to word: When a phrase is assigned to a topic, each component word in it is also promoted with a certain small count. That is, each word is also assigned the topic by a certain amount. In most cases, the head nouns are more important. Thus, we promote the head nouns more. For example, in "hotel staff", "staff" is the head noun that determines the category of the noun phrase. The rationale of this promotion is similar to that above.</p><p>Let ( ) be a word and _ be the word itself or a phrase containing the word ( ) . represents a term, and _ indicates all the related terms of . The new GPU sampling is as follows:</p><formula xml:id="formula_2">( ( ) = | − , , , , , ) ∝ | + + × ∑ _ | _ , ( ) + _ ∑ ∑ _ | _ , _ +<label>(2)</label></formula><p>where is a × real-value matrix, each cell of which contains a real value virtualcount, indicating the amount of promotion of a term under a topic when assigning this topic to another term. is size of all terms. The new model retains the document-topic component of standard LDA, which is the first ratio in (1), but replaces the usual Pólya urn topic-word (topic-term) component, the second ratio in (1), with a generalized Pólya urn framework <ref type="bibr" target="#b16">(Mahmoud 2008;</ref><ref type="bibr" target="#b15">Mimno et al., 2011)</ref>. The simple Pólya urn model is a simplified version of GPU in which matrix is an identity matrix. In this paper, is an asymmetric matrix because the main goal of using GPU is to promote the less frequent phrases in the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed method of considering phrases in topic discovery, and compare it with three baselines. The first baseline discovers topics using LDA in a traditional way without considering phrases, i.e., using only individual words. We refer to this baseline as LDA(w). The second baseline considers phrases by treating each whole phrase as a separate term in the corpus. We refer to this baseline as LDA(p). The third baseline considers phrases by keeping individual component words in the phrases as they are, but also adding phrases as extra terms. We refer to this baseline as LDA(w_p). We refer to our proposed method as LDA(p_GPU). Note that for those words that are not in any phrases, they are treated as individual words (or unigrams). Data Set: We use product reviews from 30 sub-categories (types of product) in the electronics domain from Amazon.com. The sub-categories are "Camera", "Mouse", "Cellphone," etc (see the whole list below <ref type="figure">Figure 1</ref>). Each domain contains 1,000 reviews. Besides, we also use a collection of hotel reviews and a collection of restaurant reviews from TripAdvisor.com and Yelp.com. The hotel review data contains 101,234 reviews, and the restaurant review data contains 25,459 reviews. We thus have a total of 32 domains. We ran the Stanford Parser to perform sentence detection, lemmatization and POS tagging. Punctuations, stopwords, numbers and words appearing less than 5 times in each dataset are removed. Domain names are also removed, e.g., word "camera" for the domain Camera, since it co-occurs with most words in the dataset, leading to high similarity among topics/aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentences as Documents:</head><p>As noted in <ref type="bibr" target="#b19">(Titov and McDonald, 2008)</ref>, when standard topic models are applied to reviews as documents, they tend to produce topics that correspond to global properties of products (e.g., product brand name), but cannot separate different product aspects or features well. The reason is that all reviews of the same product type basically evaluate the same aspects of the product type. Only the brand names and product names are different. Thus, using individual reviews for modelling is ineffective for finding product aspects or features, which are our topics. Although there are approaches which model sentences <ref type="bibr" target="#b8">(Jo and Oh, 2011;</ref><ref type="bibr" target="#b23">Zhao et al., 2010;</ref><ref type="bibr" target="#b19">Titov and McDonald, 2008)</ref>, we take the approach in <ref type="bibr" target="#b2">(Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b4">Chen et al., 2013)</ref>, dividing each review into sentences and treating each sentence as an independent document. Noun Phrase Detection: Although there are different types of phrases, in this first work we focus only on noun phrases as they are more representative of topics in online reviews. We will deal with other types of phrases in the future. Our first step is thus to obtain all noun phrases from each domain. Due to the efficiency issue of full natural language parser with a huge number of reviews, instead of applying the Stanford Parser to recognize noun phrases, we design a rule-based approach to recognize noun phrases as consecutive nouns based on POS tags of sentences. Although the Stanford Parser may give us better noun phrases, our simple method serves the purpose and gives us very good results. In fact, based on our initial experiments, the Stanford Parser also gives many wrong phrases.</p><p>Parameter Settings: In all our experiments, the posterior inference was drawn after 2000 Gibbs sampling iterations with a burn-in of 400 iterations. Following <ref type="bibr" target="#b5">(Griffiths and Steyvers, 2004)</ref>, we fix the Dirichlet priors as follows: for all document-topic distributions, we set =50/ , where is the number of topics. And for all topic-term distributions, we set =0.1. We also experimented with other settings of these priors and did not notice much difference.</p><p>Setting the number of topics/aspects in topic models is often tricky as it is difficult to know the exact number of topics that a corpus has. While non-parametric Bayesian approaches <ref type="bibr" target="#b20">(Teh et al., 2005)</ref> do exist for estimating the number of topics, it's not the focus of this paper. We empirically set the number of topics to 15. Although 15 may not be optimum, since all models use the same number, there is no bias against any model. In Section 3.3, we introduced the promotion concept for the GPU model. When we sample a topic for a word, we add virtualcount of topic assignment to all its related phrases. However, not all words in a phrase are equally important. For example, in phrase "hotel staff", "staff" is more important, and we call such words the head nouns. In this work, we apply a simple method used in <ref type="bibr" target="#b22">(Wang et al., 2007)</ref>, which is to always assume that the last word in a noun phrase is the head noun. Although we are aware of the potential harm to our model when we promote a wrong word, we will leave it as our future work. Again, because we want to connect phrases with their component words and promote the rank of phrases in their topics, we add less virtual counts to individual words. Thus, we add 0.5 * virtualcount to the last word in a phrase and add 0.25 * virtualcount to all other words. We set virtualcount = 0.1 in our experiments empirically.</p><p>Based on the discovered topics, we conduct statistical evaluation using topic coherence, human evaluation and also a case study to quantitatively and qualitatively show the superiority of the proposed method in terms of both interpretability and topic wellness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Statistical Evaluation</head><p>Perplexity and KL-divergence are often used to evaluate topic models statistically. However, researchers have found that perplexity on held-out documents is not always a good predictor of human judgments of topics <ref type="bibr" target="#b3">(Chang et al., 2009</ref>). In our application, we are not concerned with the test on future data using the hold-out set. KL-divergence measures the difference of distributions, and thus can be used to measure the distinctiveness of topics. However, distinctiveness of topics does not necessarily mean human agreeable topics. Recently, <ref type="bibr" target="#b15">Mimno et al. (2011)</ref> proposed a new measure called topic coherence, which has been shown to correlate with human judgments of topic quality quite well. Higher topic coherence score indicates higher quality of topics, i.e., better topic coherence. Topic coherence is computed as below.</p><formula xml:id="formula_3">( ; ( ) ) = ∑ ∑ ( ( ) , ( ) ) + 1 ( ( ) ) −1 =1 =2 (3)</formula><p>in which ( ) is the document frequency of term (i.e., the number of documents with at least one term ) and ( , ′ ) is the co-document frequency of term and term ′ (i.e., the number of documents containing both term and term ′ ). Also, ( ) = ( 1 ( ) , … , ( ) ) is the list of most probable terms in topic . 1 is added as a smoothing count to avoid taking the logarithm of zero. We thus use this measure to score all four experiments. <ref type="figure">Figure 1</ref> and <ref type="figure" target="#fig_0">Figure 2</ref> show the topic coherence using top 15 terms and top 30 terms respectively on the 32 different domains. Notice the topic coherence is a negative value, and a smaller absolute value is better than a larger one. Firstly, we can see from both charts that our proposed model LDA(p_GPU) is better than all other three baselines by a large margin. Secondly, the performance of the other three baselines are quite similar. In general, LDA(p) is slightly worse than the other two baselines. It is because replacing many words with phrases decreases the number of co-occurrences in the corpus. In contrast, LDA(w_p) is slightly better than the other two baselines on most domains because some frequent phrases add more reliable co-occurrences in the corpus. However, as we point out in the introduction, some problems still exist. Firstly, it does not solve the problem of phrases and their component words having different meanings, and thus artificially creating such wrong co-occurrences may damage the overall performance. Secondly, even if the number of co-occurrences increases, most of the phrases are still too infrequent to be ranked high in their associated topics to be useful in helping users understand the topic.</p><p>In order to test the significance of the improvement, we conduct paired t-tests on the topic coherence results. Using both 15 top terms and 30 top terms, statistical tests show that our proposed method, LDA(p_GPU), outperforms all three baselines significantly (p &lt; 0.01). However, there's no significant improvement between any pair of the three baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Manual Evaluation</head><p>Although several statistical measures, such as perplexity, KL-divergence and topic coherence, have been used to statistically evaluate topic models, since topic models are mostly (including ours) unsupervised, statistical measures may not always correlate with human interpretations or judgments. Thus, in this sub-section, we perform a manual evaluation through manual labeling of topics and topical terms.</p><p>Manual labeling was done by two annotators, who are familiar with reviews and topic models. The labeling was carried out in two stages sequentially: (1) labeling of topics and (2) labeling of topical terms in each topic. After the first stage, an annotator agreement is computed and then the two annotators discuss about the disagreed topics to reach a consensus. Then, they move on to the next stage to label the top ranked topical terms in each topic (based on their probabilities in the topic). For the annotator <ref type="figure">Figure 1</ref>: Topic coherence of the top 15 terms of each model on each of the 32 datasets. Notice that since topic coherence is a negative value, a smaller absolute value is better than a larger one. Domain/dataset names are listed as follows (1:Amplifier; 2:BluRayPlayer; 3:Camera; 4:CellPhone; 5:Computer; 6:DVDPlayer; 7:GPS; 8:HardDrive; 9:Headphone; 10: <ref type="bibr">Keyboard;</ref><ref type="bibr">11:Kindle;</ref><ref type="bibr">12:MediaPlayer;</ref><ref type="bibr">13:Microphone;</ref><ref type="bibr">14:Monitor;</ref><ref type="bibr">15:Mouse;</ref><ref type="bibr">16:MP3Player;</ref><ref type="bibr">17:NetworkAdapter;</ref><ref type="bibr">18:Printer;</ref><ref type="bibr">19:Projector;</ref><ref type="bibr">20:RadarDetector;</ref><ref type="bibr">21:RemoteControl;</ref><ref type="bibr">22:Scanner;</ref><ref type="bibr">23:Speaker;</ref><ref type="bibr">24:Subwoofer;</ref><ref type="bibr">25:Tablet;</ref><ref type="bibr">26:TV;</ref><ref type="bibr">27:VideoPlayer;</ref><ref type="bibr">28:VideoRecorder;</ref><ref type="bibr">29:Watch;</ref><ref type="bibr">30:WirelessRouter;</ref><ref type="bibr">31:Hotel;</ref><ref type="bibr">32:Restaurant)</ref>.   agreement, we compute Kappa scores. The Kappa score for topic labeling is 0.838, and the Kappa score for topical terms labeling is 0.846. Both scores indicate strong agreement in the labeling. Evaluation measure. A commonly used evaluation measure in human evaluation is precision@n (or P@n for short), which is the precision at a particular rank position n in a topic. For example, Precision@5 means the precision of the top ranked 5 terms for a topic. To be consistent with the automatic evaluation, we use Precision@15 and 30. Top 15 terms is usually sufficient to represent the topic. However, since we include phrases in our experiments which may lead to some other terms ranked lower than using only words, we labeled up to top 30 terms. The Precision@n measure is also used in <ref type="bibr" target="#b23">(Zhao et al., 2010)</ref> and some others, e.g., <ref type="bibr" target="#b4">(Chen et al., 2013)</ref>.</p><p>In our experiments, we labeled four results for each domain, i.e., those of LDA(w), LDA(p), LDA(w_p) and LDA(p_GPU). Due to the large amount of human labeling effort, we only labeled 5 domains. We find that it is sometimes hard to figure out what some of the topics are about and whether some terms are related to a topic or not, so we give the results to our human evaluators together with the phrases in each domain extracted by our rules in order to let them be familiar with the domain vocabulary. The human evaluation results are shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Results and Discussions. Again, we conduct paired t-tests on the human evaluation results of top 15 and 30 terms. Statistical tests show that our proposed method, LDA(p_GPU), outperforms all other three methods significantly (p &lt; 0.05) using both top 15 and top 30 terms. However, there's no significant improvement between any pair of the three baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head><p>In order to illustrate the importance of phrases in enhancing human readability, we conduct case study using one topic from each of the five manually labeled domains. Due to space limitations, we only compare the results of our model LDA(p_GPU) with LDA(w). In the above table, we notice that with phrases, the topics are much more interpretable than only reading individual words given by LDA(w). For example, "hand" in "Watch" domain given by LDA(w) is quite confusing at first, but in LDA(p_GPU), "hour hand" makes it more understandable. Another example is "aaa" in "MP3Player" domain. It is quite confusing at first, but "aaa battery" should make it more interpretable by an application user who is not familiar with topic models or does not have extensive domain knowledge. Also, due to wrong co-occurrences created by individual words in a phrase, the LDA(w) results contain much more noise than those of LDA(p_GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper proposed a new method to consider phrases in discovering topics using topic models. The method is based on the generalized Pólya urn (GPU) model, which allows us to connect phrases with their component words during the inference and rank phrases higher in their related topics. Our method preserves the advantages of "bag-of-words" assumption while preventing the side effects that traditional methods have when considering phrases. We tested our method against three baselines across 32 different domains, and demonstrated the superiority of our method in improving the topic quality and human interpretability both quantitatively and qualitatively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Topic coherence of the top 30 terms of each model on each dataset. Notice again that since topic coherence is a negative value, a smaller absolute value is better than a larger one. X-axis indicates the domain id numbers, whose names are listed below Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Human evaluation on five domains using top 15 and top 30 terms. X-axis indicates the domain id numbers, whose corresponding domain names are listed below Figure 1. Y-axis indicates the ratio of correct topic terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>where ( ) = represents the topic assignment of term ( ) to topic , and − , refers to the topic assignments of all other terms. denotes all terms in the document collection, denotes the size of vocabulary of the collection, is the number of topics in the corpus,</figDesc><table>| is the count of term under 
topic , 
= ∑ 

′ | 

′ 

, and | refers the count of topic being assigned to some terms in document 
, 
= ∑ 

′ | 

′ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Example topics discovered by LDA(w) and LDA(p_GPU)</figDesc><table>Hotel 
Restaurant 
Watch 
LDA(w) LDA(p_GPU) LDA(w) LDA(p_GPU) 
LDA(w) LDA(p_GPU) 
bed 
clean 
service 
service 
hand 
big 
comfortable comfortable 
star 
friendly 
minute 
hand 
small 
quiet 
staff 
server 
hour 
minute 
sleep 
sleep 
atmosphere 
staff 
beautiful 
cheap 
size 
large 
friendly 
atmosphere 
casual 
hour 
large 
spacious 
server 
waiter 
christmas 
automatic 
tv 
size 
waiter 
attentive 
setting 
seconds 
pillow 
king size bed attentive 
star 
condition 
line 
king 
pillow 
reason 
service staff 
worth 
hour hand 
chair 
queen size bed decor 
star service 
weight 
durable 
table 
bed size 
quick customer service 
red 
analog hand 
mattress bed nd pillow customer 
table service 
press 
hand move 
clean 
bed sheet 
waitress delivery service 
gift 
hand line 
double 
bed linen 
tip 
rush hour service 
run 
seconds hand 
big 
sofa bed 
pleasant service attitude functionality hand sweep 
Tablet 
MP3Player 
LDA(w) 
LDA(p_GPU) 
LDA(w) 
LDA(p_GPU) 
screen 
screen 
battery 
battery 
touch 
size 
headphone 
hour 
software 
easier 
life 
battery life 
hard 
pro 
media 
price 
pad 
touch screen 
car 
worth 
option 
bigger 
windows 
charge 
version 
area 
hour 
replacement 
website 
inch 
decent 
free 
angle 
screen protector 
reason 
market 
car 
screen size 
xp 
aaa battery 
charger 
inch screen 
program 
aa battery 
ipod 
draw 
aaa 
purchase 
worth 
home screen 
window 
hour battery 
gb 
screen look 
set 
aaa 
drive 
line 
pair 
life </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Visualizing Topics with Multi-Word Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0907.1013</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">Tech. Report.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Unsupervised Aspect-Sentiment Model for Online Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading Tea Leaves: How Humans Interpret Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting Domain Knowledge in Aspect Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meichun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malu</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riddhiman</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of National Academy of Sciences</title>
		<meeting>National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Generic Approach to Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECML PKDD. ACM. Pages</title>
		<imprint>
			<biblScope unit="page" from="517" to="532" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>UAI</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aspect and Sentiment Unification Model for Online Review Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WSDM. Hong Kong</title>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Joint Sentiment/Topic Model for Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CIKM. Hong Kong</publisher>
			<pubPlace>China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sentiment Analysis with Global Topics and Local Dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Opinion Integration Through Semi-supervised Topic Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
			<pubPlace>Beijing, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic Keyphrase Extraction via Topic Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Discovering User Interactions in Ideological Discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Public Dialogue: Analysis of Tolerance in Online Discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Meraz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing Semantic Coherence in Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosan</forename><surname>Mahmoud</surname></persName>
		</author>
		<title level="m">Polya Urn Models. Chapman &amp; Hall/CRC Texts in Statistical Science</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Topic Sentiment Mixture: Modeling Facets and Opinions in Weblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wondra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
			<pubPlace>Banff, Alberta, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic Aggregation by Joint Modeling of Aspects and Values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Sauper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="89" to="127" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Modeling Online Reviews with Multi-grain Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
			<pubPlace>Beijing, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical Dirichlet Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Topic Modeling: Beyond Bag-of-Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Topical N-grams: Phrase and Topic Discovery, with an Application to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>ICDM</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jointly Modeling Aspects and Opinions with a MaxEnt-LDA Hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Topical Keyphrase Extraction from Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palakorn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Achananuparp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\Work\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-07-07T21:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unification-based Multimodal Integration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Johnston</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ira Smith Center for Human Computer Communication Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Oregon Graduate Institute</orgName>
								<address>
									<postBox>PO BOX 91000</postBox>
									<postCode>97291</postCode>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ira Smith Center for Human Computer Communication Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Oregon Graduate Institute</orgName>
								<address>
									<postBox>PO BOX 91000</postBox>
									<postCode>97291</postCode>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcgee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ira Smith Center for Human Computer Communication Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Oregon Graduate Institute</orgName>
								<address>
									<postBox>PO BOX 91000</postBox>
									<postCode>97291</postCode>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ira Smith Center for Human Computer Communication Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Oregon Graduate Institute</orgName>
								<address>
									<postBox>PO BOX 91000</postBox>
									<postCode>97291</postCode>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Pittman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ira Smith Center for Human Computer Communication Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Oregon Graduate Institute</orgName>
								<address>
									<postBox>PO BOX 91000</postBox>
									<postCode>97291</postCode>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unification-based Multimodal Integration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recent empirical research has shown conclusive advantages of multimodal interaction over speech-only interaction for mapbased tasks. This paper describes a multimodal language processing architecture which supports interfaces allowing simultaneous input from speech and gesture recognition. Integration of spoken and gestural input is driven by unification of typed feature structures representing the semantic contributions of the different modes. This integration method allows the component modalities to mutually compensate for each others' errors. It is implemented in QuickSet, a multimodal (pen/voice) system that enables users to set up and control distributed interactive simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>By providing a number of channels through which information may pass between user and computer, multimodal interfaces promise to significantly increase the bandwidth and fluidity of the interface between humans and machines. In this work, we are concerned with the addition of multimodal input to the interface. In particular, we focus on interfaces which support simultaneous input from speech and pen, utilizing speech recognition and recognition of gestures and drawings made with a pen on a complex visual display, such as a map. Our focus on multimodal interfaces is motivated, in part, by the trend toward portable computing devices for which complex graphical user interfaces are infeasible. For such devices, speech and gesture will be the primary means of user input. Recent empirical results (Oviatt 1996) demonstrate clear task performance and user preference advantages for multimodal interfaces over speech only interfaces, in particular for spatial tasks such as those involving maps. Specifically, in a within-subject experiment during which the same users performed the same tasks in various conditions using only speech, only pen, or both speech and pen-based input, users' multimodal input to maps resulted in 10% faster task completion time, 23% fewer words, 35% fewer spoken disfluencies, and 36% fewer task errors compared to unimodal spoken input. Of the user errors, 48% involved location errors on the map--errors that were nearly eliminated by the simple ability to use penbased input. Finally, 100% of users indicated a preference for multimodal interaction over speech-only interaction with maps. These results indicate that for map-based tasks, users would both perform better and be more satisfied when using a multimodal interface. As an illustrative example, in the distributed simulation application we describe in this paper, one user task is to add a "phase line" to a map. In the existing unimodal interface for this application (CommandTalk, Moore 1997), this is accomplished with a spoken utterance such as 'CRE-ATE A LINE FROM COORDINATES NINE FOUR THREE NINE THREE ONE TO NINE EIGHT NINE NINE FIVE ZERO AND CALL IT PHASE LINE GREEN'. In contrast the same task can be accomplished by saying 'PHASE LINE GREEN' and simultaneously drawing the gesture in The multimodal command involves speech recognition of only a three word phrase, while the equivalent unimodal speech command involves recognition of a complex twenty four word expression. Furthermore, using unimodal speech to indicate more com-281</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>By providing a number of channels through which information may pass between user and computer, multimodal interfaces promise to significantly increase the bandwidth and fluidity of the interface between humans and machines. In this work, we are concerned with the addition of multimodal input to the interface. In particular, we focus on interfaces which support simultaneous input from speech and pen, utilizing speech recognition and recognition of gestures and drawings made with a pen on a complex visual display, such as a map. Our focus on multimodal interfaces is motivated, in part, by the trend toward portable computing devices for which complex graphical user interfaces are infeasible. For such devices, speech and gesture will be the primary means of user input. Recent empirical results <ref type="bibr" target="#b16">(Oviatt 1996)</ref> demonstrate clear task performance and user preference advantages for multimodal interfaces over speech only interfaces, in particular for spatial tasks such as those involving maps. Specifically, in a within-subject experiment during which the same users performed the same tasks in various conditions using only speech, only pen, or both speech and pen-based input, users' multimodal input to maps resulted in 10% faster task completion time, 23% fewer words, 35% fewer spoken disfluencies, and 36% fewer task errors compared to unimodal spoken input. Of the user errors, 48% involved location errors on the map--errors that were nearly eliminated by the simple ability to use penbased input. Finally, 100% of users indicated a preference for multimodal interaction over speech-only interaction with maps. These results indicate that for map-based tasks, users would both perform better and be more satisfied when using a multimodal interface. As an illustrative example, in the distributed simulation application we describe in this paper, one user task is to add a "phase line" to a map. In the existing unimodal interface for this application <ref type="bibr">(CommandTalk, Moore 1997)</ref>, this is accomplished with a spoken utterance such as 'CRE-ATE A LINE FROM COORDINATES NINE FOUR THREE NINE THREE ONE TO NINE EIGHT NINE NINE FIVE ZERO AND CALL IT PHASE LINE GREEN'. In contrast the same task can be accomplished by saying 'PHASE LINE GREEN' and simultaneously drawing the gesture in The multimodal command involves speech recognition of only a three word phrase, while the equivalent unimodal speech command involves recognition of a complex twenty four word expression. Furthermore, using unimodal speech to indicate more com-plex spatial features such as routes and areas is practically infeasible if accuracy of shape is important.</p><p>Another significant advantage of multimodal over unimodal speech is that it allows the user to switch modes when environmental noise or security concerns make speech an unacceptable input medium, or for avoiding and repairing recognition errors <ref type="bibr" target="#b18">(Oviatt and Van Gent 1996)</ref>. Multimodality also offers the potential for input modes to mutually compensate for each others' errors. We will demonstrate :~'~.,, in our system, multimodal integration allows speech input to compensate for errors in gesture recognition and vice versa.</p><p>Systems capable of integration of speech and gesture have existed since the early 80's. One of the first such systems was the "Put-That-There" system <ref type="bibr" target="#b0">(Bolt 1980)</ref>. However, in the sixteen years since then, research on multimodal integration has not yielded a reusable scalable architecture for the construction of multimodal systems that integrate gesture and voice. There are four major limiting factors in previous approaches to multimodal integration:</p><p>(1) The majority of approaches limit the bandwidth of the gestural mode to simple deictic pointing gestures made with a mouse <ref type="bibr" target="#b15">(Neal and Shapiro 1991</ref><ref type="bibr" target="#b7">, Cohen 1991</ref><ref type="bibr" target="#b8">, Cohen 1992</ref><ref type="bibr">, Brison and Vigouroux (ms.), Wauchope 1994</ref> or with the hand <ref type="bibr">(Koons et al 19931)</ref>.</p><p>(ii) Most previous approaches have been primarily speech-driven ~ , treating gesture as a secondary dependent mode <ref type="bibr" target="#b15">(Neal and Shapiro 1991</ref><ref type="bibr" target="#b7">, Cohen 1991</ref><ref type="bibr" target="#b8">, Cohen 1992</ref>, <ref type="bibr" target="#b12">Koons et al 1993</ref><ref type="bibr" target="#b23">, Wauchope 1994</ref>. In these systems, integration of gesture is triggered by the appearance of expressions in the speech stream whose reference needs to be resolved, such as definite and deictic noun phrases (e.g. 'this one', 'the red cube').</p><p>(iii) None of the existing approaches provide a wellunderstood generally applicable common meaning representation for the different modes, or, (iv) A general and formally-welldefined mechanism for multimodal integration.</p><p>I Koons et al 1993 describe two different systems. The first uses input from hand gestures and eye gaze in order to aid in determining the reference of noun phrases in the speech stream. The second allows users to manipulate objects in a blocks world using iconic and pantomimic gestures in addition to deictic gestures. ~More precisely, they are 'verbal language'-driven. Either spoken or typed linguistic expressions are the driving force of interpretation.</p><p>We present an approach to multimodal integration which overcomes these limiting factors. A wide base of continuous gestural input is supported and integration may be driven by either mode. Typed feature structures <ref type="bibr" target="#b4">(Carpenter 1992</ref>) are used to provide a clearly defined and well understood common meaning representation for the modes, and multimodal integration is accomplished through unification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Quickset: A Multimodal Interface for Distributed Interactive Simulation</p><p>The initial application of our multimodal interface architecture has been in the development of the QuickSet system, an interface for setting up and interacting with distributed interactive simulations. QuickSet provides a portal into LeatherNet 3, a simulation system used for the training of US Marine Corps platoon leaders. LeatherNet simulates training exercises using the ModSAF simulator (Courtemanche and Ceranowicz 1995) and supports 3D visualization of the simulated exercises using CommandVu <ref type="bibr" target="#b6">(Clarkson and Yi 1996)</ref>. SRI International's CommandTalk provides a unimodal spoken interface to LeatherNet <ref type="bibr" target="#b13">(Moore et al 1997)</ref>.</p><p>QuickSet is a distributed system consisting of a collection of agents that communicate through the Open Agent Architecture 4 <ref type="bibr" target="#b9">(Cohen et al 1994)</ref>. It runs on both desktop and hand-held PCs under Windows 95, communicating over wired and wireless LANs (respectively), or modem links. The wireless hand-held unit is a 3-1b Fujitsu Stylistic 1000 ( <ref type="figure" target="#fig_1">Figure 2</ref>). We have also developed a Java-based QuickSet agent that provides a portal to the simulation over the World Wide Web. The QuickSet user interface displays a map of the terrain on which the simulated military exercise is to take place ( <ref type="figure" target="#fig_1">Figure  2</ref>). The user can gesture and draw directly on the map with the pen and simultaneously issue spoken commands. Units and objectives can be laid down on the map by speaking their name and gesturing on the desired location. The map can also be annotated with line features such as barbed wire and fortified lines, and area features such as minefields and landing zones. These are created by drawing the appropriate spatial feature on the map and speak3LeatherNet is currently being developed by the Naval Command, Control and Ocean Surveillance Center (NCCOSC) Research, Development, Test and Evaluation Division (NRaD) in coordination with a number of contractors.</p><p>4Open Agent Architecture is a trademark of SRI International. ing its name. Units, objectives, and lines can also be generated using unimodal gestures by drawing their map symbols in the desired location. Orders can be assigned to units, for example, in <ref type="figure" target="#fig_1">Figure 2</ref> an M1A1 platoon on the bottom left has been assigned a route to follow. This order is created multimodally by drawing the curved route and saying 'WHISKEY FOUR SIX FOLLOW THIS ROUTE'. As entities are created and assigned orders they are displayed on the UI and automatically instantiated in a simulation database maintained by the ModSAF simulator.</p><p>Speech recognition operates in either a click-tospeak mode, in which the microphone is activated when the pen is placed on the screen, or open microphone mode. The speech recognition agent is built using a continuous speaker-independent recognizer commercially available from IBM.</p><p>When the user draws or gestures on the map, the resulting electronic 'ink' is passed to a gesture recognition agent, which utilizes both a neural network and a set of hidden Markov models. The ink is sizenormalized, centered in a 2D image, and fed into the neural network as pixels, as well as being smoothed, resampled, converted to deltas, and fed to the HMM recognizer. The gesture recognizer currently recognizes a total of twenty six different gestures, some of which are illustrated in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O g3</head><p>Figure 4: Pen drawings of routes and areas</p><p>Another contributing factor is that users' pen input is often sloppy ( <ref type="figure" target="#fig_4">Figure 5</ref>) and map symbols can be confused among themselves and with route and area gestures. mortar tank deletion mechanized platoon company Given the potential for error, the gesture recognizer issues not just a single interpretation, but a series of potential interpretations ranked with respect to probability. The correct interpretation is frequently determined as a result of multimodal integration, as illustrated below 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>A Unification-based Architecture for Multimodal Integration</p><p>One the most significant challenges facing the development of effective multimodal interfaces concerns the integration of input from different modes. Input signals from each of the modes can be assigned meanings. The problem is to work out how to combine the meanings contribute d by each of the modes in order to determine what the user actually intends to communicate.</p><p>To model this integration, we utilize a unification operation over typed feature structures <ref type="bibr" target="#b3">(Carpenter 1990</ref><ref type="bibr" target="#b19">, Pollard and Sag 1987</ref><ref type="bibr" target="#b2">, Calder 1987</ref><ref type="bibr">, King SSee Wahlster 1991</ref> for discussion of the role of dialog in resolving ambiguous <ref type="bibr">gestures. 1989</ref><ref type="bibr">gestures. , Moshier 1988</ref>. Unification is an operation that determines the consistency of two pieces of partial information, and if they are consistent combines them into a single result. As such, it is ideally suited to the task at hand, in which we want to determine whether a given piece of gestural input is compatible with a given piece of spoken input, and if they are compatible, to combine the two inputs into a single result that can be interpreted by the system.</p><p>The use of feature structures as a semantic representation framework facilitates the specification of partial meanings. Spoken or gestural input which partially specifies a command can be represented as an underspecified feature structure in which certain features are not instantiated. The adoption of typed feature structures facilitates the statement of constraints on integration. For example, if a given speech input can be integrated with a line gesture, it can be assigned a feature structure with an underspecified location feature whose value is required to be of type line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Art I</head><p>Figure 6: Multimodal integration architecture <ref type="figure">Figure 6</ref> presents the main agents involved in the QuickSet system. Spoken and gestural input originates in the user interface client agent and it is passed on to the speech recognition and gesture recognition agents respectively. The natural language agent uses a parser implemented in Prolog to parse strings that originate from the speech recognition agent and assign typed feature structures to them. The potential interpretations of gesture from the gesture recognition agent are also represented as typed feature structures. The multimodal integration agent determines and ranks potential unifications of spoken and gestural input and issues complete commands to the bridge agent. The bridge agent accepts commands in the form of typed feature structures and translates them into commands for whichever applications the system is providing an interface to.</p><p>For example, if the user utters 'M1A1 PLA-TOON', the name of a particular type of tank platoon, the natural language agent assigns this phrase the feature structure in <ref type="figure" target="#fig_5">Figure 7</ref>. The type of each feature structure is indicated in italics at its bottom right or left corner. Since QuickSet is a task-based system directed toward setting up a scenario for simulation, this phrase is interpreted as a partially specified unit creation command. Before it can be executed, it needs a location feature indicating where to create the unit, which is provided by the user's gesturing on the screen. The user's ink is likely to be assigned a number of interpretations, for example, both a point interpretation and a line interpretation, which the gesture recognition agent assigns typed feature structures (see <ref type="figure">Figures 8 and 9</ref>). Interpretations of gestures as location features are assigned a general command type which unifies with all of commands taken by the system. The task of the integrator agent is to field incoming typed feature structures representing interpretations of speech and of gesture, identify the best potential interpretation, multimodal or unimodal, and issue a typed feature structure representing the preferred interpretation to the bridge agent, which will execute the command. This involves parsing of the speech and gesture streams in order to determine potential multimodal integrations. Two factors guide this: tagging of speech and gesture as either complete or partial and examination of time stamps associated with speech and gesture.</p><p>Speech or gesture input is marked as complete if it provides a full command specification and therefore does not need to be integrated with another mode. Speech or gesture marked as partial needs to be integrated with another mode in order to derive an executable command.</p><p>Empirical study of the nature of multimodal interaction has shown that speech typically follows gesture within a window of a three to four seconds while gesture following speech is very uncommon <ref type="bibr">(Oviatt et al 97)</ref>. Therefore, in our multimodal architecture, the integrator temporally licenses integration of speech and gesture if their time intervals overlap, or if the onset of the speech signal is within a brief time window following the end of gesture. Speech and gesture are integrated appropriately even if the integrator agent receives them in a different order from their actual order of occurrence. If speech is temporally compatible with gesture, in this respect, then the integrator takes the sets of interpretations for both speech and gesture, and for each pairing in the product set attempts to unify the two feature structures. The probability of each multimodal interpretation in the resulting set licensed by unification is determined by multiplying the probabilities assigned to the speech and gesture interpretations.</p><p>In the example case above, both speech and gesture have only partial interpretations, one for speech, and two for gesture. Since the speech interpretation ( <ref type="figure" target="#fig_5">Figure 7</ref>) requires its location feature to be of type point, only unification with the point interpretation of the gesture will succeed and be passed on as a valid multimodal interpretation <ref type="figure" target="#fig_0">(Figure 10</ref>). The ambiguity of interpretation of the gesture was resolved by integration with speech which in this case required a location feature of type point. If the spoken command had instead been 'BARBED WIRE' it would have been assigned the feature structure in <ref type="figure" target="#fig_0">Figure 11</ref>. This structure would only unify with the line interpretation of gesture resulting in the interpretation in <ref type="figure" target="#fig_0">Figure 12</ref>.  Similarly, if the spoken command described an area, for example an 'ANTI TANK MINEFIELD' , it would only unify with an interpretation of gesture as an area designation. In each case the unificationbased integration strategy compensates for errors in gesture recognition through type constraints on the values of features.</p><p>Gesture also compensates for errors in speech recognition. In the open microphone mode, where the user does not have to gesture in order to speak, spurious speech recognition errors are more common than with click-to-speak, but are frequently rejected by the system because of the absence of a compatible gesture for integration. For example, if the system spuriously recognizes 'M1A1 PLATOON', but there is no overlapping or immediately preceding gesture to provide the location, the speech will be ignored. The architecture also supports selection among nbest speech recognition results on the basis of the preferred gesture recognition. In the future, n-best recognition results will be available from the recognizer, and we will further examine the potential for gesture to help select among speech recognition alternatives.</p><p>Since speech may follow gesture, and since even simultaneously produced speech and gesture are processed sequentially, the integrator cannot execute what appears to be a complete unimodal command on receiving it, in case it is immediately followed by input from the other mode suggesting a multimodal interpretation. If a given speech or gesture input has a set of interpretations including both partial and complete interpretations, the integrator agent waits for an incoming signal from the other mode. If no signal is forthcoming from the other mode within the time window, or if interpretations from the other mode do not integrate with any interpretations in the set, then the best of the complete unimodal interpretations from the original set is sent to the bridge agent.</p><p>For example, the gesture in <ref type="figure" target="#fig_0">Figure 13</ref> is used for unimodal specification of the location of a fortified line. If recognition is successful the gesture agent would assign the gesture an interpretation like that in <ref type="figure" target="#fig_0">Figure 14.</ref> /kgXdl..O On receiving this set of interpretations, the integrator cannot immediately execute the complete interpretation to create a fortified line, even if it is assigned the highest probability by the recognizer, since speech contradicting this may immediately follow. For example, if overlapping with or just after the gesture, the user said 'BARBED WIRE' then the line feature interpretation would be preferred. If speech does not follow within the three to four second window, or following speech does not integrate with the gesture, then the unimodal interpretation is chosen. This approach embodies a preference for multimodal interpretations over unimodal ones, motivated by the possibility of unintended complete unimodal interpretations of gestures. After more detailed empirical investigation, this will be refined so that the possibility of integration weighs in favor of the multimodal interpretation, but it can still be beaten by a unimodal gestural interpretation with a significantly higher probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented an architecture for multimodal interfaces in which integration of speech and gesture is mediated and constrained by a unification operation over typed feature structures. Our approach supports a full spectrum of gestural input, not just deixis. It also can be driven by either mode and enables a wide and flexible range of interactions. Complete commands can originate in a single mode yielding unimodal spoken and gestural commands, or in a combination of modes yielding multimodal commands, in which speech and gesture are able to contribute either the predicate or the arguments of the command. This architecture allows the modes to synergistically mutual compensate for each others' errors. We have informally observed that integration with speech does succeed in resolving ambiguous gestures. In the majority of cases, gestures will have multiple interpretations, but this is rarely apparent to the user, because the erroneous interpretations of gesture are screened out by the unification process. We have also observed that in the open microphone mode multimodality allows erroneous speech recognition results to be screened out. For the application tasks described here, we have observed a reduction in the length and complexity of spoken input, compared to the unimodal spoken interface to LeatherNet, informally reconfirming the empirical results of Oviatt et al 1997. For this family of applications at least, it appears to be the case that as part of a multimodal architecture, current speech recognition technology is sufficiently robust to support easy-to-use interfaces.</p><p>Vo and Wood 1996 present an approach to multimodal integration similar in spirit to that presented here in that it accepts a variety of gestures and is not solely speech-driven. However, we believe that unification of typed feature structures provides a more general, formally well-understood, and reusable mechanism for multimodal integration than the frame merging strategy that they describe. <ref type="bibr" target="#b5">Cheyer and Julia (1995)</ref> sketch a system based on <ref type="bibr" target="#b16">Oviatt's (1996)</ref> results but describe neither the integration strategy nor multimodal compensation.</p><p>QuickSet has undergone a form of pro-active evaluation in that its design is informed by detailed predictive modeling of how users interact multimodally and it incorporates the results of existing empirical studies of multimodal interaction <ref type="bibr" target="#b16">(Oviatt 1996</ref><ref type="bibr" target="#b17">, Oviatt et al 1997</ref>. It has also undergone participatory design and user testing with the US Marine Corps at their training base at 29 Palms, California, with the US Army at the Royal Dragon exercise at Fort Bragg, North Carolina, and as part of the Command Center of the Future at NRaD.</p><p>Our initial application of this architecture has been to map-based tasks such as distributed simulation. It supports a fully-implemented usable system in which hundreds of different kinds of entities can be created and manipulated. We believe that the unification-based method described here will readily scale to larger tasks and is sufficiently general to support a wide variety of other application areas, including graphically-based information systems and editing of textual and graphical content. The architecture has already been successfully re-deployed in the construction of multimodal interface to health care information.</p><p>We are actively pursuing incorporation of statistically-derived heuristics and a more sophisticated dialogue model into the integration architecture. We are also developing a capability for automatic logging of spoken and gestural input in order to collect more fine-grained empirical data on the nature of multimodal interaction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1: Line gesture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The QuickSet user interface</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>They include var- ious military map symbols such as platoon, mortar, and fortified line, editing gestures such as deletion, and spatial features such as routes and areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example symbols and gestures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Typical pen input from real users</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Feature structure for 'M1A1 PLATOON'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Point interpretation of gesture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Multimodal interpretation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Feature structure for 'BARBED WIRE'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :Figure 14 :Figure 15 Figure 15 :</head><label>13141515</label><figDesc>Figure 13: Fortified line gesture</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgments</head><p>This work is supported in part by the Information Technology and Information Systems offices of DARPA under contract number DABT63-95-C-007, in part by ONR grant number N00014-95-1-1164, and has been done in collaboration with the US Navy's NCCOSC RDT&amp;E Division (NRaD), Ascent Technologies, Mitre Corp., MRJ Corp., and SRI International.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Put-That-There&quot; :Voice and gesture at the graphics interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Bolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="262" to="270" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multimodal references: A generic fusion process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vigouroux</surname></persName>
		</author>
		<imprint>
			<pubPlace>Toulouse, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>URIT-URA CNRS. Universit Paul Sabatier</orgName>
		</respStmt>
	</monogr>
	<note>unpublished ms</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Typed unification for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Categories, Polymorphisms, and Unification</title>
		<editor>E. Klein and J. van Benthem</editor>
		<meeting><address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
		<respStmt>
			<orgName>Centre for Cognitive Science, University of Edinburgh</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Typed feature structures: Inheritance, (In)equality, and Extensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ITK Workshop: Inheritance in Natural Language Processing</title>
		<editor>W. Daelemans and G. Gazdar</editor>
		<meeting>the ITK Workshop: Inheritance in Natural Language Processing<address><addrLine>Tilburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="9" to="18" />
		</imprint>
		<respStmt>
			<orgName>Tilburg. Institute for Language Technology and Artificial Intelligence, Tilburg University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The logic of typed feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carpenter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal maps: An agent-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Julia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cooperative Multimodal Communication (CMC/95)</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-05" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
	<note>Eindhoven</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LeatherNet: A synthetic forces tactical training system for the USMC commander</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<idno>IST-TR-96-18</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Computer Generated Forces and Behavioral Representation</title>
		<meeting>the Sixth Conference on Computer Generated Forces and Behavioral Representation</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>Institute for simulation and training</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integrated interfaces for decision support with simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Winter Simulation Conference</title>
		<editor>B. Nelson, W. D. Kelton, and G. M. Clark</editor>
		<meeting>the Winter Simulation Conference<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="1066" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The role of natural language in a multimodal interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UIST&apos;92</title>
		<meeting>UIST&apos;92<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="143" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An open agent architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Baeg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the AAA1 Spring Symposium on Software Agents</title>
		<meeting><address><addrLine>Stanford, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-03-21" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ModSAF development status</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Courtemanche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ceranowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Computer Generated Forces and Behavioral Representation</title>
		<meeting>the Fifth Conference on Computer Generated Forces and Behavioral Representation<address><addrLine>Orlando, Florida. University of Central Florida, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-05-09" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A logical formalism for head-driven phrase structure grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>Manchester, England</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Manchester</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Integrating simultaneous input from speech, gaze, and hand gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Koons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Sparrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Thorisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">M. T. Maybury, editor, Intelligent Multimedia Interfaces</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press/ MIT Press</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="257" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CommandTalk: A Spoken-Language Interface for Battlefield Simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dowding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gawron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gorfu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fifth Conference on Applied Natural Language Processing</title>
		<meeting>Fifth Conference on Applied Natural Language Processing<address><addrLine>Washington, D.C; Morristown, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Extensions to unification grammar for the description of programming languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moshier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<pubPlace>Ann Arbor, Michigan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intelligent multi-media interface technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent User Interfaces</title>
		<editor>J. W. Sullivan and S. W. Tyler</editor>
		<meeting><address><addrLine>Frontier Series; New York, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley Publishing Co</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="45" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal interfaces for dynamic interactive maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;96</title>
		<meeting>Conference on Human Factors in Computing Systems: CHI &apos;96<address><addrLine>Vancouver, Canada; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integration and synchronization of input modes during multimodal human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deangeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Factors in Computing Systems: CH[ &apos;97</title>
		<meeting>the Conference on Human Factors in Computing Systems: CH[ &apos;97<address><addrLine>Atlanta, Georgia; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Error resolution during multimodal human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Gent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Spoken Language Processing</title>
		<meeting>International Conference on Spoken Language Processing<address><addrLine>Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="204" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Informationbased syntax and semantics: Volume I, Fundamentals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Sag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSLI Lecture Notes. Center for the Study of Language and Information</title>
		<meeting><address><addrLine>Stanford, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building an application framework for speech and pen input integration in multimodal learning interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">User and discourse models for multimodal communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wahlster</surname></persName>
		</author>
		<editor>J. Sullivan and S</editor>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tyler</surname></persName>
		</author>
		<title level="m">Intelligent User Interfaces</title>
		<meeting><address><addrLine>New York, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley Publishing Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Eucalyptus: Integrating natural language input with a graphical user interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wauchope</surname></persName>
		</author>
		<idno>NRL/FR/5510-94-9711</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Naval Research Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\Work\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-07-07T21:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly Extracting Relations with Class Ties via Effective Deep Ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 30 -August 4, 2017. July 30 -August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ye</surname></persName>
							<email>yehai@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
							<email>chaowenhan@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
							<email>zhunchenluo@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Defense Science and Technology Information Center</orgName>
								<address>
									<postCode>100142</postCode>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jointly Extracting Relations with Class Ties via Effective Deep Ranking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1810" to="1820"/>
							<date type="published">July 30 -August 4, 2017. July 30 -August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/P17-1166</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Connections between relations in relation extraction, which we call class ties, are common. In distantly supervised scenario, one entity tuple may have multiple relation facts. Exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction. However, previous models are not effective or ignore to model this property. In this work, to effectively leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network (CNN) with a general pairwise ranking framework, in which three novel ranking loss functions are introduced. Additionally, an effective method is presented to relieve the severe class imbalance problem from NR (not relation) for model training. Experiments on a widely used dataset show that leveraging class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties. Our model outperforms the baselines significantly, achieving stateof-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims to classify the relations between two given named entities from natural-language text. Supervised machine learning methods require numerous labeled data to work well. With the rapid growth of volume of relation types, traditional methods can not keep up with the step for the limitation of labeled data. In order to narrow down the gap of data sparsity, <ref type="bibr" target="#b12">Mintz et al. (2009)</ref> propose distant supervision (DS) for relation extraction, which automati- cally generates training data by aligning a knowledge facts database (ie. Freebase <ref type="bibr" target="#b1">(Bollacker et al., 2008)</ref>) with texts.</p><p>Class ties mean the connections between relations in relation extraction. In general, we conclude that class ties can have two types: weak class ties and strong class ties. Weak class ties mainly involve the co-occurrence of relations such as place of birth and place lived, CEO of and founder of. On the contrary, strong class ties mean that relations have latent logical entailments. Take the two relations of capital of and city of for example, if one entity tuple has the relation of capital of, it must express the relation fact of city of, because the two relations have the entailment of capital of ⇒ city of. Obviously the opposite induction is not correct. Further take the sentence of "Jonbenet told me that her mother [Patsy Ramsey] e 1 never left [Atlanta] e 2 since she was born." in DS scenario for example. This sentence expresses two relation facts which are place of birth and place lived. However, the word "born" is a strong bios to extract place of birth, so it may not be easy to predict the relation of place lived, but if we can incorporate the weak ties between the two relations, extracting place of birth will provide evidence for prediction of place lived.</p><p>Exploiting class ties is necessary for DS based relation extraction. In DS scenario, there is a challenge that one entity tuple can have multiple rela-tion facts as shown in <ref type="table" target="#tab_0">Table 1</ref>, which is called relation overlapping <ref type="bibr" target="#b6">(Hoffmann et al., 2011;</ref><ref type="bibr" target="#b17">Surdeanu et al., 2012)</ref>. However, the relations of one entity tuple can have class ties mentioned above which can be leveraged to enhance relation extraction for it narrowing down potential searching spaces and reducing uncertainties between relations when predicting unknown relations. If one pair entities has CEO of, it will contain founder of with high possibility.</p><p>To exploit class ties between relations, we propose to make joint extraction for all positive labels of one entity tuple with considering pairwise connections between positive and negative labels inspired by <ref type="bibr" target="#b3">(Fürnkranz et al., 2008;</ref><ref type="bibr" target="#b23">Zhang and Zhou, 2006)</ref>. As the two relations with class ties shown in <ref type="table" target="#tab_0">Table 1</ref>, by joint extraction of two relations, we can maintain the class ties (co-occurrence) of them from training samples to be learned by potential model, and then leverage this learned information to extract instances with unknown relations, which can not be achieved by separated extraction for it dividing labels apart losing information of cooccurrence. To classify positive labels from negative ones, we adopt pairwise ranking to rank positive ones higher than negative ones, exploiting pairwise connections between them. In a word, joint extraction exploits class ties between relations and pairwise ranking classify positive labels from negative ones. Furthermore, combining information across sentences will be more appropriate for joint extraction which provides more information from other sentences to extract each relation <ref type="bibr" target="#b26">(Zheng et al., 2016;</ref><ref type="bibr" target="#b9">Lin et al., 2016)</ref>. In <ref type="table" target="#tab_0">Table  1</ref>, sentence #1 is the evidence for place of birth, but it also expresses the meaning of "living in someplace", so it can be aggregated with sentence #2 to extract place lived. Meanwhile, the word of "hometown" in sentence #2 can provide evidence for place of birth which should be combined with sentence #1 to extract place of birth.</p><p>In this work, we propose a unified model that integrates pairwise ranking with CNN to exploit class ties. Inspired by the effectiveness of deep learning for modeling sentences <ref type="bibr" target="#b8">(LeCun et al., 2015)</ref>, we use CNN to encode sentences. Similar to <ref type="bibr" target="#b15">(Santos et al., 2015;</ref><ref type="bibr" target="#b9">Lin et al., 2016)</ref>, we use class embeddings to represent relation classes. The whole model architecture is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. We first use CNN to embed sentences, then we introduce two variant methods to combine the embedded sentences into one bag representation vector aiming to aggregate information across sentences, after that we measure the similarity between bag representation and relation class in realvalued space. With two variants for combining sentences, three novel pairwise ranking loss functions are proposed to make joint extraction. Besides, to relieve the bad impact of class imbalance from NR (not relation) <ref type="bibr" target="#b7">(Japkowicz and Stephen, 2002</ref>) for training our model, we cut down loss propagation from NR class during training.</p><p>Our experimental results on dataset of <ref type="bibr" target="#b13">Riedel et al. (2010)</ref> are evident that: (1) Our model is much more effective than the baselines; (2) Leveraging class ties will enhance relation extraction and our model is efficient to learn class ties by joint extraction; (3) A much better model can be trained after relieving class imbalance from NR.</p><p>Our contributions in this paper can be encapsulated as follows:</p><p>• We propose to leverage class ties to enhance relation extraction. An effective deep ranking model which integrates CNN and pairwise ranking framework is introduced to exploit class ties.</p><p>• We propose an effective method to relieve the impact of data imbalance from NR for model training.</p><p>• Our method achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We summarize related works on two main aspects:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distant Supervision Relation Extraction</head><p>Previous works on DS based RE ignore or are not effective to leverage class ties between rela-tions. <ref type="bibr" target="#b13">Riedel et al. (2010)</ref> introduce multi-instance learning to relieve the wrong labelling problem, ignoring class ties. Afterwards, <ref type="bibr" target="#b6">Hoffmann et al. (2011)</ref> and <ref type="bibr" target="#b17">Surdeanu et al. (2012)</ref> model this problem by multi-instance multi-label learning to extract overlapping relations. Though they also propose to make joint extraction of relations, they only use information from single sentence losing information from other sentences. <ref type="bibr" target="#b4">Han and Sun (2016)</ref> try to use Markov logic model to capture consistency between relation labels, on the contrary, our model leverages deep ranking to learn class ties automatically.</p><p>With the remarkable success of deep learning in CV and NLP <ref type="bibr" target="#b8">(LeCun et al., 2015)</ref>, deep learning has been applied to relation extraction <ref type="bibr" target="#b21">(Zeng et al., 2014</ref><ref type="bibr" target="#b20">(Zeng et al., , 2015</ref><ref type="bibr" target="#b15">Santos et al., 2015;</ref><ref type="bibr" target="#b9">Lin et al., 2016)</ref>, the specific deep learning architecture can be CNN <ref type="bibr" target="#b21">(Zeng et al., 2014)</ref>, RNN , etc. <ref type="bibr" target="#b20">Zeng et al. (2015)</ref> propose a piecewise convolutional neural network with multi-instance learning for DS based relation extraction, which improves the precision and recall significantly. Afterwards, <ref type="bibr" target="#b9">Lin et al. (2016)</ref> introduce the mechanism of attention <ref type="bibr" target="#b11">(Luong et al., 2015;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref> to select the sentences to relieve the wrong labelling problem and use all the information across sentences. However, the two deep learning based models only make separated extraction thus can not model class ties between relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning to Rank</head><p>Deep learning to rank has been widely used in many problems to serve as a classification model. In image retrieval,  apply deep semantic ranking for multi-label image retrieval. In text matching, <ref type="bibr" target="#b16">Severyn and Moschitti (2015)</ref> adopt learning to rank combined with deep CNN for short text pairs matching. In traditional supervised relation extraction, <ref type="bibr" target="#b15">Santos et al. (2015)</ref> design a pairwise loss function based on CNN for single label relation extraction. Based on the advantage of deep learning to rank, we propose pairwise learning to rank (LTR) <ref type="bibr" target="#b10">(Liu, 2009</ref>) combined with CNN in our model aiming to jointly extract multiple relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>In this section, we first conclude the notations used in this paper, then we introduce the used CNN for sentence embedding, afterwards, we present our algorithm of how to learn class ties between relations of one entity tuple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We define the relation classes as L = {1, 2, · · · , C}, entity tuples as</p><formula xml:id="formula_0">T = {t i } M i=1 and mentions 1 as X = {x i } N i=1</formula><p>. Dataset is constructed as follows: for entity tuple t i ∈ T and its relation class set L i ⊆ L, we collect all the mentions X i that contain t i , the dataset we use is</p><formula xml:id="formula_1">D = {(t i , L i , X i )} H i=1 . Given a data (t k , L k , X k ) ∈ {(t i , L i , X i )} H i=1</formula><p>, the sentence embeddings of X k encoded by CNN are defined as</p><formula xml:id="formula_2">S k = {s i } |X k | i=1</formula><p>and we use class embeddings W ∈ R |L|×d to represent the relation classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CNN for Sentence Embedding</head><p>We take the effective CNN architecture adopted from <ref type="bibr" target="#b20">(Zeng et al., 2015;</ref><ref type="bibr" target="#b9">Lin et al., 2016)</ref> to encode sentence and we briefly introduce CNN in this section. More details of our CNN can be obtained from previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Words Representations</head><p>• Word Embedding Given a word embedding matrix V ∈ R l w ×d 1 where l w is the size of word dictionary and d 1 is the dimension of word embedding, the words of a mention x = {w 1 , w 2 , · · · , w n } will be represented by realvalued vectors from V .</p><p>• Position Embedding The position embedding of a word measures the distance from the word to entities in a mention. We add position embeddings into words representations by appending position embedding to word embedding for every word. Given a position embedding matrix P ∈ R l p ×d 2 where l p is the number of distances and d 2 is the dimension of position embeddings, the dimension of words representations becomes</p><formula xml:id="formula_3">d w = d 1 + d 2 × 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Convolution, Piecewise max-pooling</head><p>After transforming words in x to real-valued vectors, we get the sentence q ∈ R n×d w . The set of kernels K is</p><formula xml:id="formula_4">{K i } d s i=1</formula><p>where d s is the number of kernels. Define the window size as d win and given one kernel K k ∈ R d win ×d w , the convolution operation is defined as follows:</p><formula xml:id="formula_5">m [i] = q [i:i+d win −1] K k + b [k]</formula><p>( <ref type="formula">1)</ref> where m is the vector after conducting convolution along q for n − </p><formula xml:id="formula_6">z [j] = max(m [p j−1 :p j ] )<label>(2)</label></formula><p>where z ∈ R 3 is the result of mention x processed by kernel K k ; 1 ≤ j ≤ 3. Given the set of kernels K, following the above steps, the mention x can be embedded to o where o ∈ R d s * 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Non-Linear Layer, Regularization</head><p>To learn high-level features of mentions, we apply a non-linear layer after pooling layer. After that, a dropout layer is applied to prevent overfitting. We define the final fixed sentence representation as s</p><formula xml:id="formula_7">∈ R d f (d f = d s * 3). s = g(o) • h<label>(3)</label></formula><p>where g(·) is a non-linear function and we use tanh(·) in this paper; h is a Bernoulli random vector with probability p to be 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Class Ties by Joint Extraction with Pairwise Ranking</head><p>As mentioned above, to learn class ties, we propose to make joint extraction with considering pairwise connections between positive labels and negative ones. Pairwise ranking is applied to achieve this goal. Besides, combining information across sentences is necessary for joint extraction. More specifically, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, from down to top, all information from sentences is pre-propagated to provide enough information for joint extraction. From top to down, pairwise ranking jointly extracting positive relations by combining losses, which are back-propagated to CNN to learn class ties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Combining Information across Sentences</head><p>We propose two options to combine sentences to provide enough information for joint extraction. • AVE The first option is average method. This method regards all the sentences equally and directly average the values in all dimensions of sentence embedding. This AVE function is defined as follows:</p><formula xml:id="formula_8">s = 1 n s i ∈S k s i<label>(4)</label></formula><p>where n is the number of sentences and s is the representation vector combining all sentence embeddings. Because it weights the importance of sentences equally, this method may bring much noise data from two aspects: (1) the wrong labelling data; (2) irrelated mentions for one relation class, for all sentences containing the same entity tuple being combined together to construct the bag representation.</p><p>• ATT The second one is a sentence-level attention algorithm used by <ref type="bibr" target="#b9">Lin et al. (2016)</ref> to measure the importance of sentences aiming to relieve the wrong labelling problem. For every sentence, ATT will calculate a weight by comparing the sentence to one relation. We first calculate the similarity between one sentence embedding and relation class as follows:</p><formula xml:id="formula_9">e j = a · W [c] · s j<label>(5)</label></formula><p>where e j is the similarity between sentence embedding s j and relation class c and a is a bias factor. In this paper, we set a as 0.5. Then we apply</p><p>Softmax to rescale e (e = {e i }</p><formula xml:id="formula_10">|X k | i=1 ) to [0, 1].</formula><p>We get the weight α j for s j as follows:</p><formula xml:id="formula_11">α j = exp(e j ) e i ∈e exp(e i )<label>(6)</label></formula><p>so the function to merge s with ATT is as follows:</p><formula xml:id="formula_12">s = |X k | i=1 α i · s i (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Joint Extraction by Combining Losses to Learn Class Ties</head><p>Firstly, we have to present the score function to measure the similarity between s and relation c.</p><p>• Score Function We use dot function to produce score for s to be predicted as relation c. The score function is as follows:</p><formula xml:id="formula_13">F(s, c) = W [c] · s<label>(8)</label></formula><p>There are other options for score function. In , they propose a margin based loss function that measures the similarity between s and W <ref type="bibr">[c]</ref> by distance. Because score function is not an important issue in our model, we adopt dot function, also used by Santos et al. <ref type="formula" target="#formula_6">(2015)</ref> and <ref type="bibr" target="#b9">Lin et al. (2016)</ref>, as our score function. Now we start to introduce the ranking loss function.</p><p>Pairwise ranking aims to learn the score function F(s, c) that ranks positive classes higher than negative ones. This goal can be summarized as follows:</p><formula xml:id="formula_14">∀c + ∈ L k , ∀c − ∈ L−L k : F(s, c + ) &gt; F(s, c − )+β<label>(9)</label></formula><p>where β is a margin factor which controls the minimum margin between the positive scores and negative scores.</p><p>To learn class ties between relations, we extend the formula (9) to make joint extraction and we propose three ranking loss functions with variants of combining sentences. Followings are the proposed loss functions:</p><p>• with AVE (Variant-1) We define the marginbased loss function with option of AVE to aggregate sentences as follows:</p><formula xml:id="formula_15">G [ave] = c + ∈L k ρ[0, σ + − F(s, c + )] + +ρ|L k |[0, σ − + F(s, c − )] +<label>(10)</label></formula><p>where [0, · ] + = max(0, · ); ρ is the rescale factor, σ + is positive margin and σ − is negative margin. Similar to <ref type="bibr" target="#b15">Santos et al. (2015)</ref> and , this loss function is designed to rank positive classes higher than negative ones controlled by the margin of σ + − σ − . In reality, F(s, c + ) will be higher than σ + and F(s, c − ) will be lower than σ − . In our work, we set ρ as 2, σ + as 2.5 and σ − as 0.5 adopted from <ref type="bibr" target="#b15">Santos et al. (2015)</ref>. Similar to <ref type="bibr" target="#b19">Weston et al. (2011) and</ref><ref type="bibr" target="#b15">Santos et al. (2015)</ref>, we update one negative class at every training round but to balance the loss between positive classes and negative ones, we multiply |L k | before the right term in function (10) to expand the negative loss. We apply mini-bach based stochastic gradient descent (SGD) to minimize the loss function. The negative class is chosen as the one with highest score among all negative classes <ref type="bibr" target="#b15">(Santos et al., 2015)</ref>, i.e.:</p><formula xml:id="formula_16">c − = argmax c∈L−L k F(s, c)<label>(11)</label></formula><p>• with ATT (Variant-2) Now we define the loss function for the option of ATT to combine sentences as follows:</p><formula xml:id="formula_17">G [att] = c + ∈L k (ρ[0, σ + − F(s c + , c + )] + +ρ[0, σ − + F(s c + , c − )] + )<label>(12)</label></formula><p>where s c means the attention weights of representation s are merged by comparing sentence embeddings with relation class c and c − is chosen by the following function:</p><formula xml:id="formula_18">c − = argmax c∈L−L k F(s c + , c)<label>(13)</label></formula><p>which means we update one negative class in every training round. We keep the values of ρ, σ + and σ − same as values in function (10). According to this loss function, we can see that: for each class c + ∈ L k , it will capture the most related information from sentences to merge s c + , then rank F(s c + , c + ) higher than all negative scores which each is F(</p><formula xml:id="formula_19">s c + , c − ) (c − ∈ L − L k ).</formula><p>We use the same update algorithm to minimize this loss.</p><p>• Extended with ATT (Variant-3) According to function (12), for each c + , we only select one negative class to update the parameters, which only considers the connections between positive classes and negative ones, ignoring connections between positive classes, so we extend function (12) to better exploit class ties by considering the connections between positive classes. We give out the extended loss function as follows:</p><formula xml:id="formula_20">G [Exatt] = c * ∈L k ( c + ∈L k ρ[0, σ + − F(s c * , c + )] + +ρ[0, σ − + F(s c * , c − )] + )<label>(14)</label></formula><p>Pro.  Similar to function (13), we select c − as follows:</p><formula xml:id="formula_21">c − = argmax c∈L−L k F(s c * , c)<label>(15)</label></formula><p>and we use the same method to update this loss function as discussed above. From the function (14), we can see that: for c * ∈ L k , after merging the bag representation s with c * , we share s with all the other positive classes and update the class embeddings of other positive classes with s, in this way, the connections between positive classes can be captured and learned by our model. In loss function <ref type="formula" target="#formula_15">(10)</ref>, <ref type="formula" target="#formula_6">(12)</ref> and <ref type="formula" target="#formula_8">(14)</ref>, we combine losses from all positive labels to make joint extraction to capture the class ties among relations. Suppose we make separated extraction, the losses from positive labels will be divided apart and will not get enough information of connections between positive labels, comparing to joint extraction. Connections between positive labels and negative ones are exploited by controlling margins: σ + and σ − .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relieving Impact of NR</head><p>In relation extraction, the dataset will always contain certain negative samples which do not express relations classified as NR (not relation). Table 2 presents the proportion of NR samples in SemEval-2010 Task 8 dataset 2 (Erk and Strapparava, 2010) and dataset from <ref type="bibr" target="#b13">Riedel et al. (2010)</ref>, which shows almost data is about NR in the latter dataset. Data imbalance will severely affect the model training and cause the model only sensitive to classes with high proportion <ref type="bibr" target="#b5">(He and Garcia, 2009)</ref>.</p><p>In order to relieve the impact of NR in DS based relation extraction, we cut the propagation of loss from NR, which means if relation c is NR, we set its loss as 0. Our method is similar to <ref type="bibr" target="#b15">Santos et al. (2015)</ref> with slight variance. <ref type="bibr" target="#b15">Santos et al. (2015)</ref> directly omit the NR class embedding, but we keep it. If we use ATT method to combine information across sentences, we can not omit NR class</p><formula xml:id="formula_22">Algorithm 1: Merging loss function of Variant-3 input : L, (t k , L k , X k ) and S k ; output: G [Exatt] ; 1 G [Exatt] ← 0; 2 for c * ∈ L k do 3</formula><p>Merge representation s c * by function <ref type="formula" target="#formula_9">(5)</ref>, <ref type="formula" target="#formula_11">(6)</ref>, <ref type="formula">(7)</ref>;</p><formula xml:id="formula_23">4 for c + ∈ L k do 5 if c + is not NR then 6 G [Exatt] ← G [Exatt] + ρ[0, σ + − F(s c * , c + )] + ; 7 c − ← argmax c∈L−L k F(s c * , c); 8 G [Exatt] ← G [Exatt] + ρ[0, σ − + F(s c * , c − )] + ; 9 return G [Exatt] ;</formula><p>embedding according to function <ref type="formula" target="#formula_11">(6)</ref> and <ref type="formula">(7)</ref>, on the contrary, it will be updated from the negative classes' loss.</p><p>In Algorithm 1, we give out the pseudocodes of merging loss with Variant-3 and considering to relieve the impact of NR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Criteria</head><p>We conduct our experiments on a widely used dataset, developed by <ref type="bibr" target="#b13">Riedel et al. (2010)</ref> and has been used by <ref type="bibr" target="#b6">Hoffmann et al. (2011</ref><ref type="bibr" target="#b17">), Surdeanu et al. (2012</ref>, <ref type="bibr" target="#b20">Zeng et al. (2015)</ref> and <ref type="bibr" target="#b9">Lin et al. (2016)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Word Embeddings. We use a word2vec tool that is gensim 3 to train word embeddings on NYT corpus. Similar to <ref type="bibr" target="#b9">Lin et al. (2016)</ref>, we keep the words that appear more than 100 times to construct word dictionary and use "UNK" to represent the other ones.  <ref type="table">Table 3</ref>: Hyper-parameter settings.</p><p>Hyper-parameter Settings. Three-fold validation on the training dataset is adopted to tune the parameters following <ref type="bibr" target="#b17">Surdeanu et al. (2012)</ref>. We use grid search to determine the optimal hyperparameters. We select word embedding size from {50, 100, 150, 200, 250, 300}. Batch size is tuned from {80, 160, 320, 640}. We determine learning rate among {0.01, 0.02, 0.03, 0.04}. The window size of convolution is tuned from {1, 3, 5}. We keep other hyper-parameters same as <ref type="bibr" target="#b20">Zeng et al. (2015)</ref>: the number of kernels is 230, position embedding size is 5 and dropout rate is 0.5. <ref type="table">Table 3</ref> shows the detailed parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with Baselines</head><p>Baseline. We compare our model with the following baselines:</p><p>• Mintz <ref type="bibr" target="#b12">(Mintz et al., 2009</ref>) the original distantly supervised model.</p><p>• MultiR (Hoffmann et al., 2011) a multiinstance learning based graphical model which aims to address overlapping relation problem.</p><p>• MIML <ref type="bibr" target="#b17">(Surdeanu et al., 2012</ref>) also solving overlapping relations in a multi-instance multilabel framework.</p><p>• PCNN+ATT <ref type="bibr" target="#b9">(Lin et al., 2016</ref>) the state-ofthe-art model in dataset of <ref type="bibr" target="#b13">Riedel et al. (2010)</ref> which applies ATT to combine the sentences. Results and Discussion. We compare our three variants of loss functions with the baselines and the results are shown in <ref type="figure">Figure 3</ref>. From the results we can see that: (1) Rank + AVE (Variant-1) achieves comparable results with PCNN+ATT; (2) Rank + ATT (Variant-2) and Rank + ExATT (Variant-3) significantly outperform PCNN + ATT with much higher precision and slightly higher recall in whole view; (3) Rank + ExATT (Variant-3) exhibits the best performances comparing with all the other methods including PCNN + ATT, Rank + AVE and Rank + ATT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Joint Extraction and Class Ties</head><p>In this section, we conduct experiments to reveal the effectiveness of our model to learn class ties with three variant loss functions mentioned above, and the impact of class ties for relation extraction. As mentioned above, we make joint extraction to learn class ties, so to achieve the goal of this set of experiments, we compare joint extraction with separated extraction. To make separated extraction, we divide the labels of entity tuple into single label and for one relation label we only select the sentences expressing this relation, then we use this dataset to train our model with the three variant loss functions. We conduct experiments with Rank + AVE (Variant-1), Rank + ATT (Variant-2) and Rank + ExATT (Variant-3) relieving impact of NR. Aggregated P/R curves are drawn and precisions@N (100, 200, · · · , 500) are reported to show the model performances.  Experimental results are shown in <ref type="figure">Figure 4</ref> and <ref type="table">Table 4</ref>. From the results we can see that: (1) For Rank + ATT and Rank + ExATT, joint extraction exhibits better performance than separated extraction, which demonstrates class ties will improve relation extraction and the two methods are effective to learn class ties; (2) For Rank + AVE, surprisingly joint extraction does not keep up with separated extraction. For the second phenomenon, the explanation may lie in the AVE method to aggregate sentences will incorporate noise data consistent with the finding in <ref type="bibr" target="#b9">Lin et al. (2016)</ref>. When make joint extraction, we will combine all sentences containing the same entity tuple no matter which class type is expressed, so it will engender much noise if we only combine them equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P@N(%)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparisons of Variant Joint Extractions</head><p>To make joint extraction, we have proposed three variant loss functions including Rank + AVE, Rank + ATT and Rank + ExATT in the above discussion and <ref type="figure">Figure 3</ref> shows that the three variants achieve different performances. In this experiment, we aim to compare the three variants in detail. We conduct the experiments with the three variants under the setting of relieving im-   pact of NR and joint extraction. We draw the P/R curves and report the top N (100, 200, · · · , 500) precisions to compare model performance with the three variants.</p><p>From the results as shown in <ref type="figure" target="#fig_4">Figure 5</ref> and Table 5 we can see that: (1) Comparing Rank + AVE with Rank + ATT, from the whole view, they can achieve the similar maximal recall point, but Rank + ATT exhibits higher precision in all range of recall; (2) Comparing Rank + ATT with Rank + ExATT, Rank + ExATT achieves much better performance with broader range of recall and higher precision in almost range of recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Impact of NR Relation</head><p>The goal of this experiment is to inspect how much relation of NR can affect the model performance. We use Rank + AVE, Rank + ATT, Rank + ExATT under the setting of relieving impact of NR or not to conduct experiments. We draw the aggregated P/R curves as shown in <ref type="figure" target="#fig_5">Figure 6</ref>, from which we can see that after relieving the impact of NR, the model performance can be improved significantly.</p><p>Then we further evaluate the impact of NR for convergence behavior of our model in model train- ing. Also with the three variant loss functions, in each iteration, we record the maximal value of Fmeasure 4 to represent the model performance at current epoch. Model parameters are tuned for 15 times and the convergence curves are shown in <ref type="figure" target="#fig_6">Figure 7</ref>. From the result, we can find out: "+NR" converges quicker than "-NR" and arrives to the final score at the around 11 or 12 epoch. In general, "-NR" converges more smoothly and will achieve better performance than "+NR" in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Study</head><p>Joint vs. Sep. Extraction (Class Ties). We randomly select an entity tuple (Cuyahoga County, Cleveland) from test set to see its scores for every relation class with the method of Rank + ATT under the setting of relieving impact of NR with joint extraction and separated extraction. This entity tuple have two relations: /location/./county seat and /location/./contains, which derive from the same root class and they have weak class ties for they all relating to topic of "location". We rescale the scores by adding value 10. The results are shown in <ref type="figure" target="#fig_7">Figure 8</ref>, from which we can see that: under joint extraction setting, the two gold relations have the highest scores among the other relations but under separated extraction setting, only /location/./contains can be distinguished from the negative relations, which demonstrates that joint extraction is better than separated extraction by capturing the class ties between relations. 4 F = 2 * P * R/(P + R) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Works</head><p>In this paper, we leverage class ties to enhance relation extraction by joint extraction using pairwise ranking combined with CNN. An effective method is proposed to relieve the impact of NR for model training. Experimental results on a widely used dataset show that leveraging class ties will enhance relation extraction and our model is effective to learn class ties. Our method significantly outperforms the baselines.</p><p>In the future, we will focus on two aspects: (1) Our method in this paper considers pairwise intersections between labels, so to better exploit class ties, we will extend our method to exploit all other labels' influences on each relation for relation extraction, transferring second-order to high-order <ref type="bibr" target="#b24">(Zhang and Zhou, 2014)</ref>; (2) We will focus on other problems by leveraging class ties between labels, specially on multi-label learning problems <ref type="bibr" target="#b28">(Zhou et al., 2012)</ref> such as multi-category text categorization <ref type="bibr" target="#b14">(Rousu et al., 2005)</ref> and multi-label image categorization <ref type="bibr" target="#b22">(Zha et al., 2008)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The main architecture of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of mechanism of our model to model class ties between relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. The dataset aligns Freebase relation facts with the New York Times corpus, in which train- ing mentions are from 2005-2006 corpus and test mentions from 2007. Following Mintz et al. (2009), we adopt held- out evaluation framework in all experiments. Ag- gregated precision/recall curves are drawn and precision@N (P@N) is reported to illustrate the model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Performance comparison of our model and the baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results for comparisons of variant joint extractions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results for impact of relation NR with methods of Rank + AVE, Rank + ATT and Rank + ExATT. "+NR" means not relieving impact of NR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Impact of NR for model convergence. "+NR" means not relieving NR impact; "-NR" is opposite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The output scores for every relation with method of Rank + ATT. The top is under joint extraction setting; the bottom is under separated extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Training instances generated by freebase.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>d win + 1 times and b ∈ R d s is the bias vector. For these vectors whose indexes out of range of [1, n], we replace them with zero vectors. By piecewise max-pooling, when pooling, the sentence is divided into three parts: m [p 0 :p 1 ] , m [p 1 :p 2 ] and m [p 2 :p 3 ] (p 1 and p 2 are the positions of entities, p 0 is the beginning of sentence and p 3 is the end of sentence). This piecewise max-pooling is defined as follows:</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The proportions of NR samples from SemEval-2010 Task 8 dataset and Riedel dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>R.+ExATT 83.5 82.2 78.7 77.2 73.1 79.0</figDesc><table>P@N(%) 
100 200 300 400 500 Ave. 
R.+AVE 
81.3 76.4 74.6 69.6 66.0 73.6 
R.+ATT 
87.9 84.3 78.0 74.9 70.3 79.1 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Precisions for top 100, 200, 300, 400, 500 and average of them for Rank + AVE, Rank + ATT and Rank + ExATT.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The sentence containing one certain entity is called mention.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is a dataset for relation extraction in traditional supervision framework.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://radimrehurek.com/gensim/models/word2vec.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Firstly, we would like to thank Xianpei Han and Kang Liu for their valuable suggestions on the initial version of this paper, which have helped a lot to improve the paper. Secondly, we also want to express gratitudes to the anonymous reviewers for their hard work and kind comments, which will further improve our work in the future. This work was supported by the National High-tech Research and Development Program (863 Program) (No. 2014AA015105) and National Natural Science Foundation of China (No. 61602490).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Proceedings of SemEval. The Association for Computer Linguistics</title>
		<editor>Katrin Erk and Carlo Strapparava</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilabel classification via calibrated label ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="153" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2950" to="2956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The class imbalance problem: A systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaju</forename><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent data analysis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="429" to="449" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP. Association for Computational Linguistics</title>
		<meeting>ACL-IJCNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning hierarchical multi-category text classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ICML</title>
		<meeting>eeding of ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="744" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACL</title>
		<meeting>eeding of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WSABIE: scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of COLING</title>
		<meeting>eeding of COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint multi-label multi-instance learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Zheng-Jun Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilabel neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aggregating inter-sentence information to enhance relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Hao Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senzhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshe</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACL</title>
		<meeting>eeding of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2291" to="2320" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>

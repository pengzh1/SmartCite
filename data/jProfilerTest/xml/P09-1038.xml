<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\Work\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-07-07T19:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phrase-Based Statistical Machine Translation as a Traveling Salesman Problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-08">August 2009. 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Zaslavskiy</surname></persName>
							<email>mikhail.zaslavskiy@ensmp.fr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Mines ParisTech</orgName>
								<orgName type="department" key="dep2">Institut Curie Xerox Research Centre Europe</orgName>
								<address>
									<postCode>77305, 38240</postCode>
									<settlement>Fontainebleau, Meylan</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
							<email>marc.dymetman@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Mines ParisTech</orgName>
								<orgName type="department" key="dep2">Institut Curie Xerox Research Centre Europe</orgName>
								<address>
									<postCode>77305, 38240</postCode>
									<settlement>Fontainebleau, Meylan</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
							<email>nicola.cancedda@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Mines ParisTech</orgName>
								<orgName type="department" key="dep2">Institut Curie Xerox Research Centre Europe</orgName>
								<address>
									<postCode>77305, 38240</postCode>
									<settlement>Fontainebleau, Meylan</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Phrase-Based Statistical Machine Translation as a Traveling Salesman Problem</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACL and AFNLP</title>
						<meeting>the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP <address><addrLine>Suntec, Singapore, 2-7</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="333" to="341"/>
							<date type="published" when="2009-08">August 2009. 2009</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>An efficient decoding algorithm is a crucial element of any statistical machine translation system. Some researchers have noted certain similarities between SMT decoding and the famous Traveling Salesman Problem; in particular <ref type="bibr" target="#b10">(Knight, 1999)</ref> has shown that any TSP instance can be mapped to a sub-case of a word-based SMT model, demonstrating NP-hardness of the decoding task. In this paper, we focus on the reverse mapping, showing that any phrase-based SMT decoding problem can be directly reformulated as a TSP. The transformation is very natural, deepens our understanding of the decoding problem, and allows direct use of any of the powerful existing TSP solvers for SMT decoding. We test our approach on three datasets, and compare a TSP-based decoder to the popular beam-search algorithm. In all cases, our method provides competitive or better performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Phrase-based systems <ref type="bibr" target="#b11">(Koehn et al., 2003)</ref> are probably the most widespread class of Statistical Machine Translation systems, and arguably one of the most successful. They use aligned sequences of words, called biphrases, as building blocks for translations, and score alternative candidate translations for the same source sentence based on a log-linear model of the conditional probability of target sentences given the source sentence:</p><formula xml:id="formula_0">p(T, a|S) = 1 Z S exp k λ k h k (S, a, T )<label>(1)</label></formula><p>where the h k are features, that is, functions of the source string S, of the target string T , and of the alignment a, where the alignment is a representation of the sequence of biphrases that where used in order to build T from S; The λ k 's are weights and Z S is a normalization factor that guarantees that p is a proper conditional probability distribution over the pairs (T, A). Some features are local, i.e. decompose over biphrases and can be precomputed and stored in advance. These typically include forward and reverse phrase conditional probability features log p(t|s) as well as log p(s|t), wheres is the source side of the biphrase andt the target side, and the so-called "phrase penalty" and "word penalty" features, which count the number of phrases and words in the alignment. Other features are non-local, i.e. depend on the order in which biphrases appear in the alignment. Typical non-local features include one or more n-gram language models as well as a distortion feature, measuring by how much the order of biphrases in the candidate translation deviates from their order in the source sentence. Given such a model, where the λ i 's have been tuned on a development set in order to minimize some error rate (see e.g. <ref type="bibr" target="#b12">(Lopez, 2008)</ref>), together with a library of biphrases extracted from some large training corpus, a decoder implements the actual search among alternative translations:</p><p>(a * , T * ) = arg max (a,T )</p><formula xml:id="formula_1">P (T, a|S).<label>(2)</label></formula><p>The decoding problem (2) is a discrete optimization problem. Usually, it is very hard to find the exact optimum and, therefore, an approximate solution is used. Currently, most decoders are based on some variant of a heuristic left-to-right search, that is, they attempt to build a candidate translation (a, T ) incrementally, from left to right, extending the current partial translation at each step with a new biphrase, and computing a score composed of two contributions: one for the known elements of the partial translation so far, and one a heuristic estimate of the remaining cost for completing the translation. The variant which is mostly used is a form of beam-search, where several partial candidates are maintained in parallel, and candidates for which the current score is too low are pruned in favor of candidates that are more promising.</p><p>We will see in the next section that some characteristics of beam-search make it a suboptimal choice for phrase-based decoding, and we will propose an alternative. This alternative is based on the observation that phrase-based decoding can be very naturally cast as a Traveling Salesman Problem (TSP), one of the best studied problems in combinatorial optimization. We will show that this formulation is not only a powerful conceptual device for reasoning on decoding, but is also practically convenient: in the same amount of time, off-the-shelf TSP solvers can find higher scoring solutions than the state-of-the art beam-search decoder implemented in Moses <ref type="bibr" target="#b7">(Hoang and Koehn, 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Beam-search decoding In beam-search decoding, candidate translation prefixes are iteratively extended with new phrases. In its most widespread variant, stack decoding, prefixes obtained by consuming the same number of source words, no matter which, are grouped together in the same stack 1 and compete against one another. Threshold and histogram pruning are applied: the former consists in dropping all prefixes having a score lesser than the best score by more than some fixed amount (a parameter of the algorithm), the latter consists in dropping all prefixes below a certain rank.</p><p>While quite successful in practice, stack decoding presents some shortcomings. A first one is that prefixes obtained by translating different subsets of source words compete against one another. In one early formulation of stack decoding for SMT <ref type="bibr" target="#b4">(Germann et al., 2001)</ref>, the authors indeed proposed to lazily create one stack for each subset of source words, but acknowledged issues with the potential combinatorial explosion in the number of stacks. This problem is reduced by the use of heuristics for estimating the cost of translating the remaining part of the source sentence. How-ever, this solution is only partially satisfactory. On the one hand, heuristics should be computationally light, much lighter than computing the actual best score itself, while, on the other hand, the heuristics should be tight, as otherwise pruning errors will ensue. There is no clear criterion to guide in this trade-off. Even when good heuristics are available, the decoder will show a bias towards putting at the beginning the translation of a certain portion of the source, either because this portion is less ambiguous (i.e. its translation has larger conditional probability) or because the associated heuristics is less tight, hence more optimistic. Finally, since the translation is built left-to-right the decoder cannot optimize the search by taking advantage of highly unambiguous and informative portions that should be best translated far from the beginning. All these reasons motivate considering alternative decoding strategies.</p><p>Word-based SMT and the TSP As already mentioned, the similarity between SMT decoding and TSP was recognized in <ref type="bibr" target="#b10">(Knight, 1999)</ref>, who focussed on showing that any TSP can be reformulated as a sub-class of the SMT decoding problem, proving that SMT decoding is NP-hard. Following this work, the existence of many efficient TSP algorithms then inspired certain adaptations of the underlying techniques to SMT decoding for word-based models. Thus, <ref type="bibr" target="#b4">(Germann et al., 2001</ref>) adapt a TSP subtour elimination strategy to an IBM-4 model, using generic Integer Programming techniques. The paper comes close to a TSP formulation of decoding with IBM-4 models, but does not pursue this route to the end, stating that "It is difficult to convert decoding into straight TSP, but a wide range of combinatorial optimization problems <ref type="bibr">(including TSP)</ref> can be expressed in the more general framework of linear integer programming". By employing generic IP techniques, it is however impossible to rely on the variety of more efficient both exact and approximate approaches which have been designed specifically for the TSP. In <ref type="bibr" target="#b16">(Tillmann and Ney, 2003)</ref> and <ref type="bibr" target="#b17">(Tillmann, 2006)</ref>, the authors modify a certain Dynamic Programming technique used for TSP for use with an IBM-4 word-based model and a phrase-based model respectively. However, to our knowledge, none of these works has proposed a direct reformulation of these SMT models as TSP instances. We believe we are the first to do so, working in our case with the mainstream phrase-based SMT models, and therefore making it possible to directly apply existing TSP solvers to SMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Traveling Salesman Problem and its variants</head><p>In this paper the Traveling Salesman Problem appears in four variants: STSP. The most standard, and most studied, variant is the Symmetric TSP: we are given a nondirected graph G on N nodes, where the edges carry real-valued costs. The STSP problem consists in finding a tour of minimal total cost, where a tour (also called Hamiltonian Circuit) is a "circular" sequence of nodes visiting each node of the graph exactly once;</p><p>ATSP. The Asymmetric TSP, or ATSP, is a variant where the underlying graph G is directed and where, for i and j two nodes of the graph, the edges (i,j) and (j,i) may carry different costs.</p><p>SGTSP. The Symmetric Generalized TSP, or SGTSP: given a non-oriented graph G of |G| nodes with edges carrying real-valued costs, given a partition of these |G| nodes into m non-empty, disjoint, subsets (called clusters), find a circular sequence of m nodes of minimal total cost, where each cluster is visited exactly once.</p><p>AGTSP. The Asymmetric Generalized TSP, or AGTSP: similar to the SGTSP, but G is now a directed graph.</p><p>The STSP is often simply denoted TSP in the literature, and is known to be NP-hard <ref type="bibr" target="#b1">(Applegate et al., 2007)</ref>; however there has been enormous interest in developing efficient solvers for it, both exact and approximate.</p><p>Most of existing algorithms are designed for STSP, but ATSP, SGTSP and AGTSP may be reduced to STSP, and therefore solved by STSP algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reductions AGTSP→ATSP→STSP</head><p>The transformation of the AGTSP into the ATSP, introduced by <ref type="bibr" target="#b13">(Noon and Bean, 1993)</ref>), is illustrated in <ref type="figure" target="#fig_0">Figure (1)</ref>. In this diagram, we assume that Y 1 , . . . , Y K are the nodes of a given cluster, while X and Z are arbitrary nodes belonging to other clusters. In the transformed graph, we introduce edges between the Y i 's in order to form a cycle as shown in the figure, where each edge has a large negative cost −K. We leave alone the incoming edge to Y i from X, but the outgoing edge from Y i to X has its origin changed to Y i−1 . A feasible tour in the original AGTSP problem passing through X, Y i , Z will then be "encoded" as a tour of the transformed graph that first traverses X , then traverses Y i , . . . , Y K , . . . , Y i−1 , then traverses Z (this encoding will have the same cost as the original cost, minus (k − 1)K). Crucially, if K is large enough, then the solver for the transformed ATSP graph will tend to traverse as many K edges as possible, meaning that it will traverse exactly k − 1 such edges in the cluster, that is, it will produce an encoding of some feasible tour of the AGTSP problem.</p><p>As for the transformation ATSP→STSP, several variants are described in the literature, e.g. <ref type="bibr">(Applegate et al., 2007, p. 126)</ref>; the one we use is from <ref type="bibr" target="#b18">(Wikipedia, 2009</ref>) (not illustrated here for lack of space).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TSP algorithms</head><p>TSP is one of the most studied problems in combinatorial optimization, and even a brief review of existing approaches would take too much place. Interested readers may consult <ref type="bibr" target="#b1">(Applegate et al., 2007;</ref><ref type="bibr" target="#b6">Gutin, 2003)</ref> for good introductions.</p><p>One of the best existing TSP solvers is implemented in the open source Concorde package <ref type="bibr" target="#b0">(Applegate et al., 2005)</ref>. Concorde includes the fastest exact algorithm and one of the most efficient implementations of the Lin-Kernighan (LK) heuristic for finding an approximate solution. LK works by generating an initial random feasible solution for the TSP problem, and then repeatedly identifying an ordered subset of k edges in the current tour and an ordered subset of k edges not included in the tour such that when they are swapped the objective function is improved. This is somewhat reminiscent of the Greedy decoding of <ref type="bibr" target="#b4">(Germann et al., 2001</ref>), but in LK several transformations can be applied simultaneously, so that the risk of being stuck in a local optimum is reduced <ref type="bibr">(Applegate et al., 2007, chapter 15)</ref>.</p><p>As will be shown in the next section, phrasebased SMT decoding can be directly reformulated as an AGTSP. Here we use Concorde through first transforming AGTSP into STSP, but it might also be interesting in the future to use algorithms specifically designed for AGTSP, which could improve efficiency further (see Conclusion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Phrase-based Decoding as TSP</head><p>In this section we reformulate the SMT decoding problem as an AGTSP. We will illustrate the approach through a simple example: translating the French sentence "cette traduction automatique est curieuse" into English. We assume that the relevant biphrases for translating the sentence are as follows: Under this model, we can produce, among others, the following translations:</p><formula xml:id="formula_2">h · mt · i · s this machine translation is strange h · c · t · i · a this curious translation is automatic ht · s · i · a</formula><p>this translation strange is automatic where we have indicated on the left the ordered sequence of biphrases that leads to each translation. We now formulate decoding as an AGTSP, in the following way. The graph nodes are all the possible pairs (w, b), where w is a source word in the source sentence s and b is a biphrase containing this source word. The graph clusters are the subsets of the graph nodes that share a common source word w.</p><p>The costs of a transition between nodes M and N of the graph are defined as follows: • The cost associated with the features local to b in the biphrase library;</p><p>• The "distortion" cost of consuming the source word w just after the source word w: |pos(w ) − pos(w) − 1|, where pos(w) and pos(w ) are the positions of w and w in the source sentence.</p><p>• The language model cost of producing the target words of b right after the target words of b; with a bigram language model, this cost can be precomputed directly from b and b . This restriction to bigram models will be removed in Section 4.1.</p><p>(c) In all other cases, the transition cost is infinite, or, in other words, there is no edge in the graph between M and N . A special cluster containing a single node (denoted by $-$$ in the figures), and corresponding to special beginning-of-sentence symbols must also be included: the corresponding edges and weights can be worked out easily. <ref type="figure" target="#fig_2">Figures 2 and 3</ref> give some illustrations of what we have just described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">From Bigram to N-gram LM</head><p>Successful phrase-based systems typically employ language models of order higher than two. However, our models so far have the following important "Markovian" property: the cost of a path is additive relative to the costs of transitions. For example, in the example of <ref type="figure" target="#fig_3">Figure 3</ref>, the cost of this · machine translation · is · strange, can only take into account the conditional probability of the word strange relative to the word is, but not relative to the words translation and is. If we want to extend the power of the model to general n-gram language models, and in particular to the 3-gram  case (on which we concentrate here, but the techniques can be easily extended to the general case), the following approach can be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compiling Out for Trigram models</head><p>This approach consists in "compiling out" all biphrases with a target side of only one word. We replace each biphrase b with single-word target side by "extended" biphrases b 1 , . . . , b r , which are "concatenations" of b and some other biphrase b in the library. <ref type="bibr">2</ref> To give an example, consider that we: (1) remove from the biphrase library the biphrase i, which has a single word target, and (2) add to the library the extended biphrases mti, ti, si, . . ., that is, all the extended biphrases consisting of the concatenation of a biphrase in the library with i, then it is clear that these extended biphrases will provide enough context to compute a trigram probability for the target word produced immediately next (in the examples, for the words strange,  automatic and automatic respectively). If we do that exhaustively for all biphrases (relevant for the source sentence at hand) that, like i, have a singleword target, we will obtain a representation that allows a trigram language model to be computed at each point.</p><p>The situation becomes clearer by looking at <ref type="figure" target="#fig_5">Figure 4</ref>, where we have only eliminated the biphrase i, and only shown some of the extended biphrases that now encapsulate i, and where we show one valid circuit. Note that we are now able to associate with the edge connecting the two nodes (est, mti) and (curieuse, s) a trigram cost because mti provides a large enough target context. While this exhaustive "compiling out" method works in principle, it has a serious defect: if for the sentence to be translated, there are m relevant biphrases, among which k have single-word targets, then we will create on the order of km extended biphrases, which may represent a significant overhead for the TSP solver, as soon as k is large relative to m, which is typically the case. The problem becomes even worse if we extend the compiling-out method to n-gram language models with n &gt; 3. In the Future Work section below, we describe a powerful approach for circumventing this problem, but with which we have not experimented yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Monolingual word re-ordering</head><p>In the first series of experiments we consider the artificial task of reconstructing the original word order of a given English sentence. First, we randomly permute words in the sentence, and then we try to reconstruct the original order by max- imizing the LM score over all possible permutations. The reconstruction procedure may be seen as a translation problem from "Bad English" to "Good English". Usually the LM score is used as one component of a more complex decoder score which also includes biphrase and distortion scores. But in this particular "translation task" from bad to good English, we consider that all "biphrases" are of the form e − e, where e is an English word, and we do not take into account any distortion: we only consider the quality of the permutation as it is measured by the LM component. Since for each "source word" e, there is exactly one possible "biphrase" e − e each cluster of the Generalized TSP representation of the decoding problem contains exactly one node; in other terms, the Generalized TSP in this situation is simply a standard TSP. Since the decoding phase is then equivalent to a word reordering, the LM score may be used to compare the performance of different decoding algorithms. Here, we compare three different algorithms: classical beamsearch (Moses); a decoder based on an exact TSP solver (Concorde); a decoder based on an approximate TSP solver (Lin-Kernighan as implemented in the Concorde solver) 3 . In the Beam-search and the LK-based TSP solver we can control the trade-off between approximation quality and running time. To measure re-ordering quality, we use two scores. The first one is just the "internal" LM score; since all three algorithms attempt to maximize this score, a natural evaluation procedure is to plot its value versus the elapsed time. The sec-ond score is BLEU <ref type="bibr" target="#b14">(Papineni et al., 2001)</ref>, computed between the reconstructed and the original sentences, which allows us to check how well the quality of reconstruction correlates with the internal score. The training dataset for learning the LM consists of 50000 sentences from NewsCommentary corpus <ref type="bibr" target="#b3">(Callison-Burch et al., 2008)</ref>, the test dataset for word reordering consists of 170 sentences, the average length of test sentences is equal to 17 words. Bigram based reordering. First we consider a bigram Language Model and the algorithms try to find the re-ordering that maximizes the LM score. The TSP solver used here is exact, that is, it actually finds the optimal tour. <ref type="figure" target="#fig_6">Figures 5(a,b)</ref> present the performance of the TSP and Beamsearch based methods.</p><p>Trigram based reordering. Then we consider a trigram based Language Model and the algorithms again try to maximize the LM score. The trigram model used is a variant of the exhaustive compiling-out procedure described in Section 4.1. Again, we use an exact TSP solver.</p><p>Looking at <ref type="figure" target="#fig_6">Figure 5a</ref>, we see a somewhat surprising fact: the cross and some star points have positive y coordinates! This means that, when using a bigram language model, it is often possible to reorder the words of a randomly permuted reference sentence in such a way that the LM score of the reordered sentence is larger than the LM of the reference. A second notable point is that the increase in the LM-score of the beam-search with time is steady but very slow, and never reaches the level of performance obtained with the exact-TSP procedure, even when increasing the time by sev-eral orders of magnitude. Also to be noted is that the solution obtained by the exact-TSP is provably the optimum, which is almost never the case of the beam-search procedure. In <ref type="figure" target="#fig_6">Figure 5b</ref>, we report the BLEU score of the reordered sentences in the test set relative to the original reference sentences. Here we see that the exact-TSP outputs are closer to the references in terms of BLEU than the beam-search solutions. Although the TSP output does not recover the reference sentences (it produces sentences with a slightly higher LM score than the references), it does reconstruct the references better than the beam-search. The experiments with trigram language models ( <ref type="figure" target="#fig_6">Figures  5(c,d)</ref>) show similar trends to those with bigrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Translation experiments with a bigram language model</head><p>In this section we consider two real translation tasks, namely, translation from English to French, trained on Europarl <ref type="bibr" target="#b11">(Koehn et al., 2003)</ref> and translation from German to Spanish training on the NewsCommentary corpus. For Europarl, the training set includes 2.81 million sentences, and the test set 500. For NewsCommentary the training set is smaller: around 63k sentences, with a test set of 500 sentences. <ref type="figure" target="#fig_7">Figure 6</ref> presents Decoder and Bleu scores as functions of time for the two corpuses.</p><p>Since in the real translation task, the size of the TSP graph is much larger than in the artificial reordering task (in our experiments the median size of the TSP graph was around 400 nodes, sometimes growing up to 2000 nodes), directly applying the exact TSP solver would take too long; instead we use the approximate LK algorithm and compare it to Beam-Search. The efficiency of the LK algorithm can be significantly increased by using a good initialization. To compare the quality of the LK and Beam-Search methods we take a rough initial solution produced by the Beam-Search algorithm using a small value for the stack size and then use it as initial point, both for the LK algorithm and for further Beam-Search optimization (where as before we vary the Beam-Search thresholds in order to trade quality for time).</p><p>In the case of the Europarl corpus, we observe that LK outperforms Beam-Search in terms of the Decoder score as well as in terms of the BLEU score. Note that the difference between the two algorithms increases steeply at the beginning, which means that we can significantly increase the quality of the Beam-Search solution by using the LK algorithm at a very small price. In addition, it is important to note that the BLEU scores obtained in these experiments correspond to feature weights, in the log-linear model (1), that have been optimized for the Moses decoder, but not for the TSP decoder: optimizing these parameters relatively to the TSP decoder could improve its BLEU scores still further.</p><p>On the News corpus, again, LK outperforms Beam-Search in terms of the Decoder score. The situation with the BLEU score is more confuse. Both algorithms do not show any clear score improvement with increasing running time which suggests that the decoder's objective function is not very well correlated with the BLEU score on this corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>In section 4.1, we described a general "compiling out" method for extending our TSP representation to handling trigram and N-gram language models, but we noted that the method may lead to combinatorial explosion of the TSP graph. While this problem was manageable for the artificial monolingual word re-ordering (which had only one possible translation for each source word), it becomes unwieldy for the real translation experiments, which is why in this paper we only considered bigram LMs for these experiments. However, we know how to handle this problem in principle, and we now describe a method that we plan to experiment with in the future.</p><p>To avoid the large number of artificial biphrases as in 4.1, we perform an adaptive selection. Let us suppose that (w, b) is a SMT decoding graph node, where b is a biphrase containing only one word on the target side. On the first step, when we evaluate the traveling cost from (w, b) to (w , b ), we take the language model component equal to</p><formula xml:id="formula_3">min b =b ,b − log p(b .v|b.e, b .e),</formula><p>where b .v represents the first word of the b target side, b.e is the only word of the b target side, and b .e is the last word of the b target size. This procedure underestimates the total cost of tour passing through biphrases that have a single-word target. Therefore if the optimal tour passes only through biphrases with more than one word on their target side, then we are sure that this tour is also optimal in terms of the tri-gram language model. Otherwise, if the optimal tour passes through (w, b), where b is a biphrase having a single-word target, we add only the extended biphrases related to b as we described in section 4.1, and then we recompute the optimal tour. Iterating this procedure provably converges to an optimal solution. This powerful method, which was proposed in <ref type="bibr" target="#b9">(Kam and Kopec, 1996;</ref><ref type="bibr" target="#b15">Popat et al., 2001)</ref> in the context of a finite-state model (but not of TSP), can be easily extended to N-gram situations, and typically converges in a small number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The main contribution of this paper has been to propose a transformation for an arbitrary phrasebased SMT decoding instance into a TSP instance. While certain similarities of SMT decoding and TSP were already pointed out in <ref type="bibr" target="#b10">(Knight, 1999)</ref>, where it was shown that any Traveling Salesman Problem may be reformulated as an instance of a (simplistic) SMT decoding task, and while certain techniques used for TSP were then adapted to word-based SMT decoding <ref type="bibr" target="#b4">(Germann et al., 2001;</ref><ref type="bibr" target="#b16">Tillmann and Ney, 2003;</ref><ref type="bibr" target="#b17">Tillmann, 2006)</ref>, we are not aware of any previous work that shows that SMT decoding can be directly reformulated as a TSP. Beside the general interest of this transformation for understanding decoding, it also opens the door to direct application of the variety of existing TSP algorithms to SMT. Our experiments on synthetic and real data show that fast TSP algorithms can handle selection and reordering in SMT comparably or better than the state-of-theart beam-search strategy, converging on solutions with higher objective function in a shorter time.</p><p>The proposed method proceeds by first constructing an AGTSP instance from the decoding problem, and then converting this instance first into ATSP and finally into STSP. At this point, a direct application of the well known STSP solver Concorde (with Lin-Kernighan heuristic) already gives good results. We believe however that there might exist even more efficient alternatives. Instead of converting the AGTSP instance into a STSP instance, it might prove better to use directly algorithms expressly designed for ATSP or AGTSP. For instance, some of the algorithms tested in the context of the DIMACS implementation challenge for ATSP <ref type="bibr" target="#b8">(Johnson et al., 2002)</ref> might well prove superior. There is also active research around AGTSP algorithms. Recently new effective methods based on a "memetic" strategy <ref type="bibr" target="#b2">(Buriol et al., 2004;</ref><ref type="bibr" target="#b5">Gutin et al., 2008)</ref> have been put forward. These methods combined with our proposed formulation provide ready-to-use SMT decoders, which it will be interesting to compare.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: AGTSP→ATSP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) If M is of the form (w, b) and N of the form (w , b), in which b is a single biphrase, and w and w are consecutive words in b, then the transition cost is 0: once we commit to using the first word of b, there is no additional cost for traversing the other source words covered by b. (b) If M = (w, b), where w is the rightmost source word in the biphrase b, and N = (w , b ), where w = w is the leftmost source word in b , then the transition cost corresponds to the cost of selecting b just after b; this will correspond to "consuming" the source side of b after having consumed the source side of b (whatever their rel- ative positions in the source sentence), and to pro- ducing the target side of b directly after the target side of b; the transition cost is then the addition of several contributions (weighted by their respective λ (not shown), as in equation 1):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Transition graph for the source sentence cette traduction automatique est curieuse. Only edges entering or exiting the node traduction − mt are shown. The only successor to [traduction − mt] is [automatique − mt], and [cette − ht] is not a predecessor of [traduction − mt].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A GTSP tours is illustrated, corresponding to the displayed output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2</head><label></label><figDesc>In the figures, such "concatenations" are denoted by [b · b] ; they are interpreted as encapsulations of first con- suming the source side of b , whether or not this source side precedes the source side of b in the source sentence, produc- ing the target side of b , consuming the source side of b, and producing the target side of b immediately after that of b .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Compiling-out of biphrase i: (est,is).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a), (b): LM and BLEU scores as functions of time for a bigram LM; (c), (d): the same for a trigram LM. The x axis corresponds to the cumulative time for processing the test set; for (a) and (c), the y axis corresponds to the mean difference (over all sentences) between the lm score of the output and the lm score of the reference normalized by the sentence length N: (LM(ref)-LM(true))/N. The solid line with star marks corresponds to using beam-search with different pruning thresholds, which result in different processing times and performances. The cross corresponds to using the exact-TSP decoder (in this case the time to the optimal solution is not under the user's control).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a), (b): Europarl corpus, translation from English to French; (c),(d): NewsCommentary corpus, translation from German to Spanish. Average value of the decoder and the BLEU scores (over 500 test sentences) as a function of time. The trade-off quality/time in the case of LK is controlled by the number of iterations, and each point corresponds to a particular number of iterations, in our experiments LK was run with a number of iterations varying between 2k and 170k. The same trade-off in the case of Beam-Search is controlled by varying the beam thresholds.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While commonly adopted in the speech and SMT communities, this is a bit of a misnomer, since the used data structures are priority queues, not stacks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Both TSP decoders may be used with/or without a distortion limit; in our experiments we do not use this parameter.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Vassilina Nikoulina for her advice about running Moses on the test datasets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Applegate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Bixby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasek</forename><surname>Chvatal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<ptr target="http://www.tsp.gatech.edu/concorde.html" />
		<title level="m">Concorde tsp solver</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Traveling Salesman Problem: A Computational Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Applegate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Bixby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasek</forename><surname>Chvatal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Princeton Series in Applied Mathematics)</title>
		<imprint>
			<date type="published" when="2007-01" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A new memetic algorithm for the asymmetric traveling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciana</forename><surname>Buriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><forename type="middle">M</forename><surname>França</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Moscato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Heuristics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="483" to="506" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Shaw Fordyce</surname></persName>
		</author>
		<title level="m">Proceedings of the Third Workshop on SMT. ACL</title>
		<meeting>the Third Workshop on SMT. ACL<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast decoding and optimal decoding for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jahr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 39</title>
		<meeting>ACL 39</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="228" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memetic algorithm for the generalized asymmetric traveling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Gutin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Karapetyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krasnogor</forename><surname>Natalio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NICSO 2007</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="199" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Travelling salesman and related problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gutin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Graph Theory</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Design of the Moses decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2008 Software workshop</title>
		<meeting><address><addrLine>Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Experimental analysis of heuristics for the atsp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gutin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Mcgeoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zverovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Travelling Salesman Problem and Its Variations</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="445" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Document image decoding by heuristic search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">E</forename><surname>Kam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="945" to="950" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decoding complexity in wordreplacement translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="607" to="615" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2003</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An efficient transformation of the generalized traveling salesman problem. INFOR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Noon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Bean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">BLEU: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">22176</biblScope>
		</imprint>
	</monogr>
	<note>IBM Research Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adding linguistic constraints to document image decoding: Comparing the iterated complete path and stack algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">H</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">K</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">S</forename><surname>Bloomberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Word reordering and a dynamic programming beam search algorithm for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="133" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient Dynamic Programming Search Algorithms For Phrase-Based SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop On Computationally Hard Problems And Joint Inference In Speech And Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Travelling Salesman ProblemWikipedia, The Free Encyclopedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online; accessed</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2009-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>

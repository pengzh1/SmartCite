<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 D:\Work\grobid-master\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-07-07T20:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local and Global Algorithms for Disambiguation to Wikipedia</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 19-24, 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<email>ddowney@eecs.northwestern.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Anderson</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Rexonomy</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Local and Global Algorithms for Disambiguation to Wikipedia</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 49th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Portland, Oregon</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1375" to="1384"/>
							<date type="published">June 19-24, 2011</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call "global" approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Wikification is the task of identifying and linking expressions in text to their referent Wikipedia pages. Recently, Wikification has been shown to form a valuable component for numerous natural language processing tasks including text classification <ref type="bibr" target="#b7">(Gabrilovich and Markovitch, 2007b;</ref><ref type="bibr">Chang et al., 2008)</ref>, measuring semantic similarity between texts <ref type="bibr" target="#b6">(Gabrilovich and Markovitch, 2007a)</ref>, crossdocument co-reference resolution , and other tasks <ref type="bibr" target="#b10">(Kulkarni et al., 2009</ref>).</p><p>Previous studies on Wikification differ with respect to the corpora they address and the subset of expressions they attempt to link. For example, some studies focus on linking only named entities, whereas others attempt to link all "interesting" expressions, mimicking the link structure found in Wikipedia. Regardless, all Wikification systems are faced with a key Disambiguation to Wikipedia (D2W) task. In the D2W task, we're given a text along with explicitly identified substrings (called mentions) to disambiguate, and the goal is to output the corresponding Wikipedia page, if any, for each mention. For example, given the input sentence "I am visiting friends in &lt;Chicago&gt;," we output http://en.wikipedia.org/wiki/Chicago -the Wikipedia page for the city of Chicago, Illinois, and not (for example) the page for the 2002 film of the same name.</p><p>Local D2W approaches disambiguate each mention in a document separately, utilizing clues such as the textual similarity between the document and each candidate disambiguation's Wikipedia page. Recent work on D2W has tended to focus on more sophisticated global approaches to the problem, in which all mentions in a document are disambiguated simultaneously to arrive at a coherent set of disambiguations <ref type="bibr" target="#b2">(Cucerzan, 2007;</ref><ref type="bibr" target="#b14">Milne and Witten, 2008b;</ref><ref type="bibr" target="#b8">Han and Zhao, 2009</ref>). For example, if a mention of "Michael Jordan" refers to the computer scientist rather than the basketball player, then we would expect a mention of "Monte Carlo" in the same document to refer to the statistical technique rather than the location. Global approaches utilize the Wikipedia link graph to estimate coherence. 1375 In this paper, we analyze global and local approaches to the D2W task. Our contributions are as follows: <ref type="formula" target="#formula_0">(1)</ref> We present a formulation of the D2W task as an optimization problem with local and global variants, and identify the strengths and the weaknesses of each, (2) Using this formulation, we present a new global D2W system, called GLOW. In experiments on existing and novel D2W data sets, 1 GLOW is shown to outperform the previous stateof-the-art system of <ref type="bibr" target="#b14">(Milne and Witten, 2008b)</ref>, <ref type="formula" target="#formula_2">(3)</ref> We present an error analysis and identify the key remaining challenge: determining when mentions refer to concepts not captured in Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition and Approach</head><p>We formalize our Disambiguation to Wikipedia (D2W) task as follows. We are given a document d with a set of mentions M = {m 1 , . . . , m N }, and our goal is to produce a mapping from the set of mentions to the set of Wikipedia titles W = {t 1 , . . . , t |W | }. Often, mentions correspond to a concept without a Wikipedia page; we treat this case by adding a special null title to the set W .</p><p>The D2W task can be visualized as finding a many-to-one matching on a bipartite graph, with mentions forming one partition and Wikipedia titles the other (see <ref type="figure" target="#fig_0">Figure 1)</ref>. We denote the output matching as an N -tuple Γ = (t 1 , . . . , t N ) where t i is the output disambiguation for mention m i . <ref type="bibr">1</ref> The data sets are available for download at http://cogcomp.cs.illinois.edu/Data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Local and Global Disambiguation</head><p>A local D2W approach disambiguates each mention m i separately. Specifically, let φ(m i , t j ) be a score function reflecting the likelihood that the candidate title t j ∈ W is the correct disambiguation for m i ∈ M . A local approach solves the following optimization problem:</p><formula xml:id="formula_0">Γ * local = arg max Γ N i=1 φ(m i , t i )<label>(1)</label></formula><p>Local D2W approaches, exemplified by <ref type="bibr" target="#b0">(Bunescu and Pasca, 2006)</ref> and <ref type="bibr" target="#b12">(Mihalcea and Csomai, 2007)</ref>, utilize φ functions that assign higher scores to titles with content similar to that of the input document.</p><p>We expect, all else being equal, that the correct disambiguations will form a "coherent" set of related concepts. Global approaches define a coherence function ψ, and attempt to solve the following disambiguation problem:</p><formula xml:id="formula_1">Γ * = arg max Γ [ N i=1 φ(m i , t i ) + ψ(Γ)]<label>(2)</label></formula><p>The global optimization problem in Eq. 2 is NPhard, and approximations are required <ref type="bibr" target="#b2">(Cucerzan, 2007)</ref>. The common approach is to utilize the Wikipedia link graph to obtain an estimate pairwise relatedness between titles ψ(t i , t j ) and to efficiently generate a disambiguation context Γ ′ , a rough approximation to the optimal Γ * . We then solve the easier problem:</p><formula xml:id="formula_2">Γ * ≈ arg max Γ N i=1 [φ(m i , t i ) + t j ∈Γ ′ ψ(t i , t j )]<label>(3)</label></formula><p>Eq. 3 can be solved by finding each t i and then mapping m i independently as in a local approach, but still enforces some degree of coherence among the disambiguations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Wikipedia was first explored as an information source for named entity disambiguation and information retrieval by <ref type="bibr" target="#b0">Bunescu and Pasca (2006</ref>  <ref type="bibr" target="#b0">(Bunescu and Pasca, 2006)</ref> and <ref type="bibr" target="#b12">(Mihalcea and Csomai, 2007)</ref> fall into the local framework. Subsequent work on Wikification has stressed that assigned disambiguations for the same document should be related, introducing the global approach <ref type="bibr" target="#b2">(Cucerzan, 2007;</ref><ref type="bibr" target="#b14">Milne and Witten, 2008b;</ref><ref type="bibr" target="#b8">Han and Zhao, 2009;</ref><ref type="bibr" target="#b4">Ferragina and Scaiella, 2010)</ref>. The two critical components of a global approach are the semantic relatedness function ψ between two titles, and the disambiguation context Γ ′ . In <ref type="bibr" target="#b14">(Milne and Witten, 2008b)</ref>, the semantic context is defined to be a set of "unambiguous surface forms" in the text, and the title relatedness ψ is computed as Normalized Google Distance (NGD) <ref type="bibr" target="#b1">(Cilibrasi and Vitanyi, 2007)</ref>. <ref type="bibr">2</ref> On the other hand, in <ref type="bibr" target="#b2">(Cucerzan, 2007</ref>) the disambiguation context is taken to be all plausible disambiguations of the named entities in the text, and title relatedness is based on the overlap in categories and incoming links. Both approaches have limitations. The first approach relies on the pres-ence of unambiguous mentions in the input document, and the second approach inevitably adds irrelevant titles to the disambiguation context. As we demonstrate in our experiments, by utilizing a more accurate disambiguation context, GLOW is able to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System Architecture</head><p>In this section, we present our global D2W system, which solves the optimization problem in Eq. 3. We refer to the system as GLOW, for Global Wikification. We use GLOW as a test bed for evaluating local and global approaches for D2W. GLOW combines a powerful local model φ with an novel method for choosing an accurate disambiguation context Γ ′ , which as we show in our experiments allows it to outperform the previous state of the art.</p><p>We represent the functions φ and ψ as weighted sums of features. Specifically, we set:</p><formula xml:id="formula_3">φ(m, t) = i w i φ i (m, t)<label>(4)</label></formula><p>where each feature φ i (m, t) captures some aspect of the relatedness between the mention m and the Wikipedia title t. Feature functions ψ i (t, t ′ ) are defined analogously. We detail the specific feature functions utilized in GLOW in following sections. The coefficients w i are learned using a Support Vector Machine over bootstrapped training data from Wikipedia, as described in Section 4.5. At a high level, the GLOW system optimizes the objective function in Eq. 3 in a two-stage process. We first execute a ranker to obtain the best non-null disambiguation for each mention in the document, and then execute a linker that decides whether the mention should be linked to Wikipedia, or whether instead switching the top-ranked disambiguation to null improves the objective function. As our experiments illustrate, the linking task is the more challenging of the two by a significant margin.</p><p>Figure 2 provides detailed pseudocode for GLOW. Given a document d and a set of mentions M , we start by augmenting the set of mentions with all phrases in the document that could be linked to Wikipedia, but were not included in M . Introducing these additional mentions provides context that may be informative for the global coherence computation (it has no effect on local approaches). In the second 1377  step, we construct for each mention m i a limited set of candidate Wikipedia titles T i that m i may refer to. Considering only a small subset of Wikipedia titles as potential disambiguations is crucial for tractability (we detail which titles are selected below). In the third step, the ranker outputs the most appropriate non-null disambiguation t i for each mention m i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm: Disambiguate to Wikipedia</head><formula xml:id="formula_4">Input: document d, Mentions M = {m1, . . . , mN } Output: a disambiguation Γ = (t1, . . . , tN ). 1) Let M ′ = M ∪ { Other potential mentions in d} 2) For each mention m ′ i ∈ M ′ , construct a set of disam- biguation candidates Ti = {t i 1 , . . . , t i k i }, t i j = null 3) Ranker: Find a solution Γ = (t ′ 1 , . . . , t ′ |M ′ | ), where t ′ i ∈ Ti is the best non-null disambiguation of m ′ i . 4) Linker: For each m ′ i , map t ′ i to null in Γ iff</formula><p>In the final step, the linker decides whether the top-ranked disambiguation is correct. The disambiguation (m i , t i ) may be incorrect for several reasons: (1) mention m i does not have a corresponding Wikipedia page, (2) m i does have a corresponding Wikipedia page, but it was not included in T i , or (3) the ranker erroneously chose an incorrect disambiguation over the correct one.</p><p>In the below sections, we describe each step of the GLOW algorithm, and the local and global features utilized, in detail. Because we desire a system that can process documents at scale, each step requires trade-offs between accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Disambiguation Candidates Generation</head><p>The first step in GLOW is to extract all mentions that can refer to Wikipedia titles, and to construct a set of disambiguation candidates for each mention. Following previous work, we use Wikipedia hyperlinks to perform these steps. GLOW utilizes an anchortitle index, computed by crawling Wikipedia, that maps each distinct hyperlink anchor text to its target Wikipedia titles. For example, the anchor text "Chicago" is used in Wikipedia to refer both to the city in Illinois and to the movie. Anchor texts in the index that appear in document d are used to supplement the mention set M in Step 1 of the GLOW algorithm in <ref type="figure" target="#fig_2">Figure 2</ref>. Because checking all substrings  in the input text against the index is computationally inefficient, we instead prune the search space by applying a publicly available shallow parser and named entity recognition system. <ref type="bibr">3</ref> We consider only the expressions marked as named entities by the NER tagger, the noun-phrase chunks extracted by the shallow parser, and all sub-expressions of up to 5 tokens of the noun-phrase chunks.</p><p>To retrieve the disambiguation candidates T i for a given mention m i in Step 2 of the algorithm, we query the anchor-title index. T i is taken to be the set of titles most frequently linked to with anchor text m i in Wikipedia. For computational efficiency, we utilize only the top 20 most frequent target pages for the anchor text; the accuracy impact of this optimization is analyzed in Section 6.</p><p>From the anchor-title index, we compute two local features φ i (m, t). The first, P (t|m), is the fraction of times the title t is the target page for an anchor text m. This single feature is a very reliable indicator of the correct disambiguation <ref type="bibr" target="#b3">(Fader et al., 2009</ref>), and we use it as a baseline in our experiments. The second, P (t), gives the fraction of all Wikipedia articles that link to t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Local Features φ</head><p>In addition to the two baseline features mentioned in the previous section, we compute a set of text-based local features φ(t, m). These features capture the intuition that a given Wikipedia title t is more likely to be referred to by mention m appearing in document d if the Wikipedia page for t has high textual similarity to d, or if the context surrounding hyperlinks to t are similar to m's context in d.</p><p>For each Wikipedia title t, we construct a top-200 token TF-IDF summary of the Wikipedia page t, which we denote as T ext(t) and a top-200 token TF-IDF summary of the context within which t was hyperlinked to in Wikipedia, which we denote as Context(t). We keep the IDF vector for all tokens in Wikipedia, and given an input mention m in a document d, we extract the TF-IDF representation of d, which we denote T ext(d), and a TF-IDF representation of a 100-token window around m, which we denote Context(m). This allows us to define four local features described in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>We additionally compute weighted versions of the features described above. Error analysis has shown that in many cases the summaries of the different disambiguation candidates for the same surface form s were very similar. For example, consider the disambiguation candidates of "China' and their TF-IDF summaries in <ref type="figure" target="#fig_0">Figure 1</ref>. The majority of the terms selected in all summaries refer to the general issues related to <ref type="bibr">China, such as "legalism, reform, military, control, etc."</ref>, while a minority of the terms actually allow disambiguation between the candidates. The problem stems from the fact that the TF-IDF summaries are constructed against the entire Wikipedia, and not against the confusion set of disambiguation candidates of m. Therefore, we re-weigh the TF-IDF vectors using the TF-IDF scheme on the disambiguation candidates as a adhoc document collection, similarly to an approach in <ref type="bibr" target="#b9">(Joachims, 1997)</ref> for classifying documents. In our scenario, the TF of the a token is the original TF-IDF summary score (a real number), and the IDF term is the sum of all the TF-IDF scores for the token within the set of disambiguation candidates for m. This adds 4 more "reweighted local" features in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Global Features ψ</head><p>Global approaches require a disambiguation context Γ ′ and a relatedness measure ψ in Eq. 3. In this section, we describe our method for generating a disambiguation context, and the set of global features ψ i (t, t ′ ) forming our relatedness measure.</p><p>In previous work, Cucerzan defined the disambiguation context as the union of disambiguation candidates for all the named entity mentions in the input document <ref type="bibr">(2007)</ref>. The disadvantage of this approach is that irrelevant titles are inevitably added to the disambiguation context, creating noise. Milne and Witten, on the other hand, use a set of unambiguous mentions (2008b). This approach utilizes only a fraction of the available mentions for context, and relies on the presence of unambiguous mentions with high disambiguation utility. In GLOW, we utilize a simple and efficient alternative approach: we first train a local disambiguation system, and then use the predictions of that system as the disambiguation context. The advantage of this approach is that unlike <ref type="bibr" target="#b14">(Milne and Witten, 2008b)</ref> we use all the available mentions in the document, and unlike (Cucerzan, 2007) we reduce the amount of irrelevant titles in the disambiguation context by taking only the top-ranked disambiguation per mention.</p><p>Our global features are refinements of previously proposed semantic relatedness measures between Wikipedia titles. We are aware of two previous methods for estimating the relatedness between two Wikipedia concepts: <ref type="bibr" target="#b15">(Strube and Ponzetto, 2006)</ref>, which uses category overlap, and <ref type="bibr" target="#b13">(Milne and Witten, 2008a)</ref>, which uses the incoming link structure. Previous work experimented with two relatedness measures: NGD, and Specificity-weighted Cosine Similarity. Consistent with previous work, we found NGD to be the better-performing of the two. Thus we use only NGD along with a well-known Pontwise Mutual Information (PMI) relatedness measure. Given a Wikipedia title collection W , titles t 1 and t 2 with a set of incoming links L 1 , and L 2 respectively, PMI and NGD are defined as follows:</p><formula xml:id="formula_5">N GD(L1, L2) = Log(M ax(|L1|, |L2|)) − Log(|L1 ∩ L2|) Log(|W |) − Log(M in(|L1|, |L2|)) P M I(L1, L2) = |L1 ∩ L2|/|W | |L1|/|W ||L2|/|W |</formula><p>The NGD and the PMI measures can also be computed over the set of outgoing links, and we include these as features as well. We also included a feature indicating whether the articles each link to oneanother. Lastly, rather than taking the sum of the relatedness scores as suggested by Eq. 3, we use two features: the average and the maximum relatedness to Γ ′ . We expect the average to be informative for many documents. The intuition for also including the maximum relatedness is that for longer documents that may cover many different subtopics, the maximum may be more informative than the average.</p><p>We have experimented with other semantic features, such as category overlap or cosine similarity between the TF-IDF summaries of the titles, but these did not improve performance in our experiments. The complete set of global features used in GLOW is given in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Linker Features</head><p>Given the mention m and the top-ranked disambiguation t, the linker attempts to decide whether t is indeed the correct disambiguation of m. The linker includes the same features as the ranker, plus additional features we expect to be particularly relevant to the task. We include the confidence of the ranker in t with respect to second-best disambiguation t ′ , intended to estimate whether the ranker may have made a mistake. We also include several properties of the mention m: the entropy of the distribution P (t|m), the percent of Wikipedia titles in which m appears hyperlinked versus the percent of times m appears as plain text, whether m was detected by NER as a named entity, and a Good-Turing estimate of how likely m is to be out-of-Wikipedia concept based on the counts in P (t|m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Linker and Ranker Training</head><p>We train the coefficients for the ranker features using a linear Ranking Support Vector Machine, using training data gathered from Wikipedia. Wikipedia links are considered gold-standard links for the training process. The methods for compiling the Wikipedia training corpus are given in Section 5.</p><p>We train the linker as a separate linear Support Vector Machine. Training data for the linker is obtained by applying the ranker on the training set. The mentions for which the top-ranked disambiguation did not match the gold disambiguation are treated as negative examples, while the mentions the ranker got correct serve as positive examples.   <ref type="table">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data sets and Evaluation Methodology</head><p>We evaluate GLOW on four data sets, of which two are from previous work. The first data set, from <ref type="bibr" target="#b14">(Milne and Witten, 2008b)</ref>, is a subset of the AQUAINT corpus of newswire text that is annotated to mimic the hyperlink structure in Wikipedia. That is, only the first mentions of "important" titles were hyperlinked. Titles deemed uninteresting and redundant mentions of the same title are not linked. The second data set, from <ref type="bibr" target="#b2">(Cucerzan, 2007)</ref>, is taken from MSNBC news and focuses on disambiguating named entities after running NER and co-reference resolution systems on newsire text. In this case, all mentions of all the detected named entities are linked.</p><p>We also constructed two additional data sets. The first is a subset of the ACE co-reference data set, which has the advantage that mentions and their types are given, and the co-reference is resolved. We asked annotators on Amazon's Mechanical Turk to link the first nominal mention of each co-reference chain to Wikipedia, if possible. Finding the accuracy of a majority vote of these annotations to be approximately 85%, we manually corrected the annotations to obtain ground truth for our experiments. The second data set we constructed, Wiki, is a sample of paragraphs from Wikipedia pages. Mentions in this data set correspond to existing hyperlinks in the Wikipedia text. Because Wikipedia editors explicitly link mentions to Wikipedia pages, their anchor text tends to match the title of the linked-topage-as a result, in the overwhelming majority ofcases, the disambiguation decision is as trivial as string matching. In an attempt to generate more challenging data, we extracted 10,000 random paragraphs for which choosing the top disambiguation according to P (t|m) results in at least a 10% ranker error rate. 40 paragraphs of this data was utilized for testing, while the remainder was used for training.</p><p>The data sets are summarized in <ref type="table" target="#tab_4">Table 2</ref>. The table shows the number of annotated mentions which were hyperlinked to non-null Wikipedia pages, and the number of titles in the documents (without counting repetitions). For example, the AQUAINT data set contains 727 mentions, 4 all of which refer to distinct titles. The MSNBC data set contains 747 mentions mapped to non-null Wikipedia pages, but some mentions within the same document refer to the same titles. There are 372 titles in the data set, when multiple instances of the same title within one document are not counted.</p><p>To isolate the performance of the individual components of GLOW, we use multiple distinct metrics for evaluation. Ranker accuracy, which measures the performance of the ranker alone, is computed only over those mentions with a non-null gold disambiguation that appears in the candidate set. It is equal to the fraction of these mentions for which the ranker returns the correct disambiguation. Thus, a perfect ranker should achieve a ranker accuracy of 1.0, irrespective of limitations of the candidate generator. Linker accuracy is defined as the fraction of all mentions for which the linker outputs the correct disambiguation (note that, when the title produced by the ranker is incorrect, this penalizes linker accuracy). Lastly, we evaluate our whole system against other baselines using a previously-employed "bag of titles" (BOT) evaluation <ref type="bibr" target="#b14">(Milne and Witten, 2008b)</ref>. In BOT, we compare the set of titles output for a document with the gold set of titles for that document (ignoring duplicates), and utilize standard precision, recall, and F1 measures.</p><p>In BOT, the set of titles is collected from the mentions hyperlinked in the gold annotation. That is, if the gold annotation is { (China, People's Republic of China), <ref type="bibr">(Taiwan, Taiwan)</ref>, <ref type="bibr">(Jiangsu, Jiangsu)</ref>  <ref type="table">Table 3</ref>: Percent of "solvable" mentions as a function of the number of generated disambiguation candidates. Listed is the fraction of identified mentions m whose target disambiguation t is among the top k candidates ranked in descending order of P (t|m).</p><p>and the predicted anotation is: { (China, People's Republic of China), <ref type="bibr">(China, History of China)</ref>, <ref type="bibr">(Taiwan, null)</ref>, <ref type="bibr">(Jiangsu, Jiangsu)</ref>, <ref type="bibr">(republic, Government)</ref>} , then the BOT for the gold annotation is: {People's Republic of China, Taiwan, Jiangsu} , and the BOT for the predicted annotation is: {People's Republic of China, History of China, Jiangsu} . The title Government is not included in the BOT for predicted annotation, because its associate mention republic did not appear as a mention in the gold annotation. Both the precision and the recall of the above prediction is 0.66. We note that in the BOT evaluation, following <ref type="bibr" target="#b14">(Milne and Witten, 2008b)</ref> we consider all the titles within a document, even if some the titles were due to mentions we failed to identify. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>In this section, we evaluate and analyze GLOW's performance on the D2W task. We begin by evaluating the mention detection component (Step 1 of the algorithm). The second column of <ref type="table" target="#tab_4">Table 2</ref> shows how many of the "non-null" mentions and corresponding titles we could successfully identify (e.g. out of 747 mentions in the MSNBC data set, only 530 appeared in our anchor-title index). Missing entities were primarily due to especially rare surface forms, or sometimes due to idiosyncratic capitalization in the corpus. Improving the number of identified mentions substantially is non-trivial; <ref type="bibr" target="#b16">(Zhou et al., 2010)</ref> managed to successfully identify only 59 more entities than we do in the MSNBC data set, using a much more powerful detection method based on search engine query logs. We generate disambiguation candidates for a <ref type="bibr">5</ref> We evaluate the mention identification stage in Section 6.  mention m using an anchor-title index, choosing the 20 titles with maximal P (t|m). <ref type="table">Table 3</ref> evaluates the accuracy of this generation policy. We report the percent of mentions for which the correct disambiguation is generated in the top k candidates (called "solvable" mentions). We see that the baseline prediction of choosing the disambiguation t which maximizes P (t|m) is very strong (80% of the correct mentions have maximal P (t|m) in all data sets except MSNBC). The fraction of solvable mentions increases until about five candidates per mention are generated, after which the increase is rather slow. Thus, we believe choosing a limit of 20 candidates per mention offers an attractive trade-off of accuracy and efficiency. The last column of Table 2 reports the number of solvable mentions and the corresponding number of titles with a cutoff of 20 disambiguation candidates, which we use in our experiments. Next, we evaluate the accuracy of the ranker. Table 4 compares the ranker performance with baseline, local and global features. The reweighted local features outperform the unweighted ("Naive") version, and the global approach outperforms the local approach on all data sets except Wikipedia. As the table shows, our approach of defining the disambiguation context to be the predicted disambiguations of a simpler local model ("Predictions") performs better than using NER entities as in <ref type="bibr" target="#b2">(Cucerzan, 2007)</ref>   <ref type="table">Table 6</ref>: End systems performance -BOT F1. The performance of the full system (GLOW) is similar to that of the local version. GLOW outperforms <ref type="bibr" target="#b14">(Milne and Witten, 2008b)</ref> on all data sets.</p><p>ties as in <ref type="bibr" target="#b14">(Milne and Witten, 2008b)</ref>. <ref type="bibr">6</ref> Combining the local and the global approaches typically results in minor improvements. While the global approaches are most effective for ranking, the linking problem has different characteristics as shown in <ref type="table" target="#tab_9">Table 5</ref>. We can see that the global features are not helpful in general for predicting whether the top-ranked disambiguation is indeed the correct one.</p><p>Further, although the trained linker improves accuracy in some cases, the gains are marginal-and the linker decreases performance on some data sets. One explanation for the decrease is that the linker is trained on Wikipedia, but is being tested on nonWikipedia text which has different characteristics. However, in separate experiments we found that training a linker on out-of-Wikipedia text only increased test set performance by approximately 3 percentage points. Clearly, while ranking accuracy is high overall, different strategies are needed to achieve consistently high linking performance.</p><p>A few examples from the ACE data set help il-  <ref type="table">Table 6</ref> we compare the end system BOT F1 performance. The local approach proves a very competitive baseline which is hard to beat. Combining the global and the local approach leads to marginal improvements. The full GLOW system outperforms the existing state-of-the-art system from <ref type="bibr" target="#b14">(Milne and Witten, 2008b)</ref>, denoted as M&amp;W, on all data sets. We also compared our system with the recent TAGME Wikification system <ref type="bibr" target="#b4">(Ferragina and Scaiella, 2010)</ref>. However, TAGME is designed for a different setting than ours: extremely short texts, like Twitter posts. The TAGME RESTful API was unable to process some of our documents at once. We attempted to input test documents one sentence at a time, disambiguating each sentence independently, which resulted in poor performance (0.07 points in F1 lower than the P (t|m) baseline). This happened mainly because the same mentions were linked to different titles in different sentences, leading to low precision.</p><p>An important question is why M&amp;W underperforms the baseline on the MSNBC and Wikipedia data sets. In an error analysis, M&amp;W performed poorly on the MSNBC data not due to poor disambiguations, but instead because the data set contains only named entities, which were often delimited incorrectly by M&amp;W. Wikipedia was challenging for a different reason: M&amp;W performs less well on the short (one paragraph) texts in that set, because they contain relatively few of the unambiguous entities the system relies on for disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have formalized the Disambiguation to Wikipedia (D2W) task as an optimization problem with local and global variants, and analyzed the strengths and weaknesses of each. Our experiments revealed that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.</p><p>As our error analysis illustrates, the primary remaining challenge is determining when a mention does not have a corresponding Wikipedia page. Wikipedia's hyperlinks offer a wealth of disambiguated mentions that can be leveraged to train a D2W system. However, when compared with mentions from general text, Wikipedia mentions are disproportionately likely to have corresponding Wikipedia pages. Our initial experiments suggest that accounting for this bias requires more than simply training a D2W system on a moderate number of examples from non-Wikipedia text. Applying distinct semi-supervised and active learning approaches to the task is a primary area of future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample Disambiguation to Wikipedia problem with three mentions. The mention "Jiangsu" is unambiguous. The correct mapping from mentions to titles is marked by heavy edges</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>doing so improves the objective function 5) Return Γ entries for the original mentions M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High-level pseudocode for GLOW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Baseline Feature: P (t|m), P (t) Local Features: φi(t, m) cosine-sim(Text(t),Text(m)) : Naive/Reweighted cosine-sim(Text(t),Context(m)): Naive/Reweighted cosine-sim(Context(t),Text(m)): Naive/Reweighted cosine-sim(Context(t),Context(m)): Naive/Reweighted Global Features: ψi(ti, tj) I [t i −t j ] * PMI(InLinks(ti),InLinks(tj)) : avg/max I [t i −t j ] * NGD(InLinks(ti),InLinks(tj)) : avg/max I [t i −t j ] * PMI(OutLinks(ti),OutLinks(tj)) : avg/max I [t i −t j ] * NGD(OutLinks(ti),OutLinks(tj)) : avg/max I [t i ↔t j ] : avg/max I [t i ↔t j ] * PMI(InLinks(ti),InLinks(tj)) : avg/max I [t i ↔t j ] * NGD(InLinks(ti),InLinks(tj)) : avg/max I [t i ↔t j ] * PMI(OutLinks(ti),OutLinks(tj)) : avg/max I [t i ↔t j ] * NGD(OutLinks(ti),OutLinks(tj)) : avg/max</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Ranker features. I [ti−tj ] is an indicator variable which is 1 iff t i links to t j or vise-versa. I [ti↔tj ] is 1 iff the titles point to each other.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Number of mentions and corresponding dis-
tinct titles by data set. Listed are (number of men-
tions)/(number of distinct titles) for each data set, for each 
of three mention types. Gold mentions include all dis-
ambiguated mentions in the data set. Identified mentions 
are gold mentions whose correct disambiguations exist in 
GLOW's author-title index. Solvable mentions are identi-
fied mentions whose correct disambiguations are among 
the candidates selected by GLOW (see </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>}</figDesc><table>Generated 
data sets 
Candidates k 
ACE MSNBC AQUAINT Wiki 
1 
81.69 
72.26 
91.01 
84.79 
3 
85.44 
86.22 
96.83 
94.73 
5 
86.38 
87.35 
97.17 
96.37 
20 
86.85 
88.67 
97.83 
98.59 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Ranker Accuracy. Bold values indicate the 
best performance in each feature group. The global ap-
proaches marginally outperform the local approaches on 
ranker accuracy , while combing the approaches leads to 
further marginal performance improvement. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head></head><label></label><figDesc>, or only the unambiguous enti-</figDesc><table>Data set 
Local 
Global 
Local+Global 
ACE 
80.1 → 82.8 80.6 → 80.6 
81.5 → 85.1 
MSNBC 
74.9 → 76.0 77.9 → 77.9 
76.5 → 76.9 
AQUAINT 93.5 → 91.5 93.8 → 92.1 
92.3 → 91.3 
Wiki 
92.2 → 92.0 88.5 → 87.2 
92.8 → 92.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Linker performance. The notation X → Y means that when linking all mentions, the linking accu- racy is X, while when applying the trained linker, the performance is Y . The local approaches are better suited for linking than the global approaches. The linking accu- racy is very sensitive to domain changes.</figDesc><table>System 
ACE MSNBC AQUAINT Wiki 
Baseline: P (t|m) 69.52 
72.83 
82.67 
81.77 
GLOW Local 
75.60 
74.39 
84.52 
90.20 
GLOW Global 
74.73 
74.58 
84.37 
86.62 
GLOW 
77.25 
74.88 
83.94 
90.54 
M&amp;W 
72.76 
68.49 
83.61 
80.32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head></head><label></label><figDesc>lustrate the tradeoffs between local and global fea- tures in GLOW. The global system mistakenly links "&lt;Dorothy Byrne&gt;, a state coordinator for the Florida Green Party, said . . . " to the British jour- nalist, because the journalist sense has high coher- ence with other mentions in the newswire text. How- ever, the local approach correctly maps the men- tion to null because of a lack of local contextual clues. On the other hand, in the sentence "In- stead of Los Angeles International, for example, consider flying into &lt;Burbank&gt; or John Wayne Air- port in Orange County, Calif.", the local ranker links the mention Burbank to Burbank, California, while the global system correctly maps the entity to Bob Hope Airport, because the three airports men- tioned in the sentence are highly related to one an- other. Lastly, in</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">(Milne and Witten, 2008b) also weight each mention in Γ ′ by its estimated disambiguation utility, which can be modeled by augmenting ψ on per-problem basis.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Available at http://cogcomp.cs.illinois.edu/page/software.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The data set contains votes on how important the mentions are. We believe that the results in (Milne and Witten, 2008b) were reported on mentions which the majority of annotators considered important. In contrast, we used all the mentions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In NER we used only the top prediction, because using all candidates as in (Cucerzan, 2007) proved prohibitively inefficient.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Importance of semantic representation: dataless classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06)</title>
		<editor>Ming-Wei Chang, Lev Ratinov, Dan Roth, and Vivek Srikumar</editor>
		<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06)<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="830" to="835" />
		</imprint>
	</monogr>
	<note>Proceedings of the 23rd national conference on Artificial intelligence</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The google similarity distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cilibrasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vitanyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="383" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on Wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling wikipedia-based named entity disambiguation to arbitrary web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the WikiAI 09 -IJCAI Workshop: User Contributed Knowledge and Artificial Intelligence: An Evolving Synergy</title>
		<meeting>the WikiAI 09 -IJCAI Workshop: User Contributed Knowledge and Artificial Intelligence: An Evolving Synergy<address><addrLine>Pasadena, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tagme: on-thefly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM conference on Information and knowledge management</title>
		<editor>Jimmy Huang, Nick Koudas, Gareth J. F. Jones, Xindong Wu, Kevyn Collins-Thompson, and Aijun An</editor>
		<meeting>the 19th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using Wikitology for Cross-Document Entity Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zareen</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Spring Symposium on Learning by Reading and Learning to Read</title>
		<meeting>the AAAI Spring Symposium on Learning by Reading and Learning to Read</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipediabased explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international joint conference on Artifical intelligence</title>
		<meeting>the 20th international joint conference on Artifical intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Harnessing the expertise of 70,000 human editors: Knowledge-based feature generation for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2297" to="2345" />
			<date type="published" when="2007" />
			<publisher>December</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Named entity disambiguation by leveraging wikipedia semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 18th ACM conference on Information and knowledge management, CIKM &apos;09</title>
		<meeting>eeding of the 18th ACM conference on Information and knowledge management, CIKM &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A probabilistic analysis of the rocchio algorithm with tfidf for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Machine Learning, ICML &apos;97</title>
		<meeting>the Fourteenth International Conference on Machine Learning, ICML &apos;97<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="143" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collective annotation of wikipedia entities in web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;09</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-Document Coreference Resolution: A Key Technology for Learning by Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clay</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjorie</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikesh</forename><surname>Garera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI 2009 Spring Symposium on Learning by Reading and Learning to Read</title>
		<editor>Asad Sayeed, Zareen Syed, and Ralph Weischede</editor>
		<meeting>the AAAI 2009 Spring Symposium on Learning by Reading and Learning to Read</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wikify!: linking documents to encyclopedic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, CIKM &apos;07</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management, CIKM &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An effective, low-cost measure of semantic relatedness obtained from wikipedia links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Wikipedia and AI Workshop of AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to link with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Information and knowledge management, CIKM &apos;08</title>
		<meeting>the 17th ACM conference on Information and knowledge management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wikirelate! computing semantic relatedness using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In proceedings of the 21st national conference on Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1419" to="1424" />
			<date type="published" when="2006" />
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Resolving surface forms to wikipedia topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Rouhani-Kalleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavian</forename><surname>Vasile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gaffney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="1335" to="1343" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
